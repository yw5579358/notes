# YOLO

## 📌 一物体检测评估指标

#### 1. **IOU（Intersection over Union）**

- **定义**：预测框与真实框的交并比。
  $$
  IOU= \frac{\text{预测框} \cap \text{真实框}}{\text{预测框} \cup \text{真实框}}
  $$
  
- **作用**：判断检测是否命中目标。

- **阈值设定**：通常使用 `0.5` 或 `0.75` 作为命中标准。

------

#### 2. **TP / FP / TN / FN**

这些是基于 **IOU 和类别预测** 得出的分类结果：

| 缩写 | 全称           | 含义说明                                            |
| ---- | -------------- | --------------------------------------------------- |
| TP   | True Positive  | 正确检测到目标（IOU > 阈值，且类别正确）            |
| FP   | False Positive | 错误检测（无目标却检测到了，或者 IOU 太低、类别错） |
| FN   | False Negative | 漏检（有目标但没检测到）                            |
| TN   | True Negative  | 正确未检测出非目标（在目标检测中一般不考虑）        |



> ⚠️ 在物体检测中，**TN 很少被用来评估**，因为背景区域太大，统计意义不大。

------

#### 3. **置信度（Confidence Score）**

- **定义**：模型对一个预测框的“可信度”估计，常为：
  $$
  置信度=P(该框中有物体)×P(类别)
  $$
  
- **作用**：决定是否保留该预测框。通常设一个阈值（如 0.5）筛选掉低置信度框。



#### 4. **Precision & Recall**

- **Precision（精度）**：
  $$
  Precision = \frac{TP}{TP + FP}
  $$
  表示预测为正的样本中，有多少是真的正（射出子弹的命中率，射出1个（敌人可能100个无所谓），命中1个，100%）。

- **Recall（召回率）**：
  $$
  {Recall} = \frac{TP}{TP + FN}
  $$
  表示所有正样本中，有多少被检测到了 （敌人的剿灭率，1个敌人，命中了这1个（射出100发子弹无所谓），100%）。

------

#### 5. **PR 曲线（Precision-Recall Curve）**

- 横轴是 **Recall**，纵轴是 **Precision**。
- 通过改变置信度阈值，可以得到一系列点，连成曲线。
- 曲线下的面积越大，表示模型性能越好。

------

#### 6. **AP（Average Precision）**

- **定义**：PR 曲线下的面积。常通过插值法计算。
- 一般会对每个类别分别计算 AP。

------

#### 7. **mAP（mean Average Precision）**

- **定义**：所有类别的 AP 平均值。
  $$
  {mAP} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_i
  $$
  
- **常见标准**：

  - `mAP@0.5`: IOU 阈值为 0.5。
  - `mAP@[.5:.95]`: COCO 常用指标，IOU 从 0.5 到 0.95 以 0.05 为步长取平均。



## 📌 V1版本

#### 核心思想（Core Idea）

YOLO（You Only Look Once）是一种端到端的目标检测方法，其关键思想是将目标检测任务转化为一个 **单一的回归问题**，在一张图片上 **只需要一次前向传播** 就可以同时预测出所有目标的位置和类别。不同于传统方法（如 R-CNN 系列），YOLO 不再分为候选框生成和分类两个阶段，而是**直接从图像像素到边界框和类别的预测**。

![image-20250508110349015](img\image-20250508110349015.png)

---

#### 网络架构（Architecture）

![image-20250508110539537](img\image-20250508110539537.png)

YOLOv1 的网络结构基于 GoogLeNet 的简化版本，整体由 24 个卷积层和 2 个全连接层组成。

- **输入尺寸**：448 × 448 × 3 的 RGB 图像
- **输出向量**：S × S × (B × 5 + C)
  - 默认 S=7，B=2，C=20

网络大致结构如下：

```text
输入图像 (448x448x3)
→ 24个卷积层（提取特征）
→ 2个全连接层（预测边界框 + 分类）
→ 输出 7×7×30 的特征张量 30:2个 x,y,w,h,c 和20分类
```

![image-20250508110655830](img\image-20250508110655830.png)

#### 输出解释与损失函数（Loss Function）

YOLO 将图像划分为一个 **S×S 的网格**（默认 S=7），每个网格预测 B 个边界框（B=2）以及对应的置信度和类别概率。

每个预测框包含：

- (x, y)：框的中心点相对于网格的偏移（归一化到 0~1）
- (w, h)：框的宽高（相对于整个图像，归一化）
- confidence：置信度 = P(obj) × IOU(pred, truth)

每个网格还预测 C 个类别的条件概率（P(class_i | object)）

**总损失函数 = 坐标损失 + 置信度损失 + 分类损失**

YOLOv1 的损失函数中使用了以下权重参数：

- 坐标损失乘以 λ_coord = **5**
- 没有目标的置信度损失乘以 λ_noobj = **0.5**

$$
L_total = λ_coord * Σ[object] ((x - x̂)² + (y - ŷ)² + (√w - √ŵ)² + (√h - √ĥ)²)
        + Σ[object] (C - Ĉ)²
        + λ_noobj * Σ[no object] (C - Ĉ)²
        + Σ[object] Σ(classes) (p_class - p̂_class)²
$$

√w 和 √h：使用平方根是为了让较小物体更容易优化，只回传负责预测的框（IOU 最大）中的坐标

![image-20250508110723612](img\image-20250508110723612.png)

#### 存在的问题（Limitations）

1. **定位不准**：YOLOv1 偏向于产生中心在网格中央的大目标，导致对小目标定位效果差。
2. **对重叠目标检测不理想**：每个网格只预测 2 个框，难以处理多个目标集中出现的情况。
3. **泛化能力差**：对一些未见过的对象组合（如狗骑滑板）容易误判。



## 📌V2版本

YOLOv2（又称 YOLO9000）主要解决 YOLOv1 中存在的几个问题：
- 小目标检测不准
- 定位误差大
- 泛化能力弱
- 不支持多尺度训练和检测

#### 1. **Batch Normalization**
- **目的**：加速收敛、防止过拟合。
- **应用**：舍弃Dropout，网格每一层做归一化，在所有卷积层后添加 BatchNorm。
- **效果**：mAP 提升约 **2%**

---

#### 2. **去掉全连接层 → 全卷积化**
- YOLOv1 的 FC 层参数过多、不适应变长输入。
- YOLOv2 将网络全卷积化（Fully Convolutional），输出大小可变（支持多尺度训练）。

---

#### 3. **输入图像分辨率提升 + 多尺度训练**
- V1训练时用的是224 * 224，测试时使用448 * 448，V2训练时额外又进行了10次448 * 448 的微调
- DarkNet实际输入 416x416，没有FC层，5次降采样，（13 * 13）
- **每10个epoch随机改变输入尺寸（320~608之间，步长32）**，增强模型对不同目标尺寸的鲁棒性。

![image-20250508135511389](img\image-20250508135511389.png)

- 网络在训练中习惯处理不同大小的图片 → 更强泛化能力。

---

#### 4. **使用 Anchor Boxes（先验框）**
- 吸收了 Faster R-CNN 的 anchor 思想。
- **用 K-means 聚类（IOU距离）分析标注框尺寸，得到 5 个先验框（anchors）**
  - K-means 公式修改为：距离 = 1 - IOU
- **每个 cell 不再预测 box 绝对位置，而是预测 offset**，以 anchor 为基准微调。
- 每个 grid cell 预测多个 box（默认 5 个 anchors × (5+类别)）

---

#### 5. **边界框坐标预测方式优化**
- 使用 sigmoid 限定中心偏移在 [0,1]，使预测更稳定。
- 坐标公式：
  ```text
  bx = sigmoid(tx) + cx
  by = sigmoid(ty) + cy
  bw = pw * exp(tw)
  bh = ph * exp(th)
  
  (tx, ty, tw, th) 为网络输出
  (pw, ph) 为 anchor 的宽高
  (cx, cy) 为当前 grid cell 的左上角坐标

![image-20250508112432236](img\image-20250508112432236.png)

#### 6. **改进网络结构：使用 Darknet-19**

- 原 YOLOv1 使用的是 GoogleNet 的变体。
- YOLOv2 构建了新的轻量级模型：**Darknet-19**
  - 包括 19 个卷积层 + 5 个 max pooling 层
  - 所有激活函数为 **Leaky ReLU**
  - 使用 3×3 和 1×1 卷积交替
  - FLOPs 和参数远少于 VGG 或 ResNet，速度快，精度高

#### 7. **使用高层语义与低层特征融合**

![image-20250508135405836](img\image-20250508135405836.png)

- 将 **高分辨率特征图与低层浅层特征图拼接（Passthrough Layer）**
- 类似于 Skip Connection，帮助定位小目标
- 实现方式：
  - 将中间层特征图（如26×26×512）上采样
  - 与主干网络末端的特征图（13×13×1024）拼接

![image-20250508113114790](img\image-20250508113114790.png)

![image-20250508113140611](img\image-20250508113140611.png)

![image-20250508135335247](img\image-20250508135335247.png)

| 改进点       | YOLOv1                | YOLOv2（改进）                         |
| ------------ | --------------------- | -------------------------------------- |
| 网络结构     | 基于 GoogLeNet 的变体 | 新设计的 Darknet-19，更轻更快          |
| BatchNorm    | 无                    | 所有 conv 层加入 BN，提升 2% mAP       |
| 全连接层     | 有，全连接用于预测    | 完全去除，改为全卷积                   |
| 输入分辨率   | 固定 448×448          | 多尺度训练，随机选取 320~608           |
| Anchor Boxes | 无，直接预测 box      | 引入 anchor 并通过 K-means 聚类得到    |
| 坐标回归方式 | 绝对位置              | Anchor 偏移 + Sigmoid + exp 更稳定     |
| 特征融合     | 无                    | 添加 Passthrough Layer，帮助检测小目标 |
| 感受野优化   | 无明显关注            | 更深网络 + 多尺度图像增强大感受野      |
| 微调策略     | 无                    | ImageNet 上预训练再迁移至检测任务      |

## 📌V3版本

YOLOv3 的目标是在保持 YOLO 系列实时性能的基础上，进一步提升**精度、鲁棒性、小目标检测能力**，并适配多类别检测（COCO 80类）。

| 改进点     | YOLOv2                  | YOLOv3（本次改进）                      |
| ---------- | ----------------------- | --------------------------------------- |
| 主干网络   | Darknet-19              | **Darknet-53**（残差结构 + 更深）       |
| 特征融合   | 单层融合（Passthrough） | **FPN 多层特征金字塔融合**              |
| 预测层级   | 单尺度（13×13）         | **多尺度预测：13×13, 26×26, 52×52**     |
| 分类预测   | Softmax 多类            | **独立逻辑回归 + Binary Cross Entropy** |
| 激活函数   | Leaky ReLU              | **Leaky ReLU（保留）**                  |
| 框预测方式 | Anchor + 偏移           | Anchor + 偏移（保留）                   |

---

#### 1. 核心网络：**Darknet-53**

![image-20250508143158789](img\image-20250508143158789.png)

- YOLOv3 用新的主干网络 **Darknet-53**，替代 YOLOv2 的 Darknet-19。
- 结合了 ResNet 的思想，使用大量 **残差连接（Residual Connections）**。
- 结构为：
  - **53 层卷积 + 跳连**（不含检测头），网络更深、精度更高。
  - 使用 1×1 和 3×3 卷积交替，保持效率。
  - 所有层使用 **BatchNorm + Leaky ReLU**。

Darknet-53 在 ImageNet 上精度如下：
- Top-1 Accuracy：**77.2%**
- Top-5 Accuracy：**93.8%**
- 同等精度下，推理速度比 ResNet-101 更快。

---

##### ✅ 残差连接的基本结构：

在标准的前馈网络中，输入 x会经过若干层非线性变换，得到输出 F(x)。而在残差连接中，我们不直接使用 F(x)，而是将输入 x 加到输出上：
$$
{Output} = F(x) + x
$$
这个结构叫做 **残差块（Residual Block）**

##### 📌 为什么使用残差连接？

**1. 解决深层网络的退化问题（Degradation Problem）**
 当网络层数增加时，理论上应该性能更好，但实际训练中，网络反而可能变差。残差连接能缓解这个问题，因为：

> 如果某些层不需要学习任何东西，最优解就是让 F(x)=0，网络就会自动保留输入 x，这样不会比浅层模型差。

**2. 改善梯度传播**
 残差连接类似于“旁路”或“高速通道”，可以让梯度从后面的层直接反向传播到前面的层，从而防止梯度消失或爆炸。

**3. 加快训练，提升性能**
 在图像识别、NLP（比如 Transformer）等领域都有非常明显的提升效果。

![image-20250508143758753](img\image-20250508143758753.png)

---

#### 2. 多尺度检测（Feature Pyramid）

![image-20250508142915572](img\image-20250508142915572.png)

- YOLOv2 只在一个尺度（13×13）上预测，导致对小目标识别力差。
- YOLOv3 模仿 FPN（Feature Pyramid Network）设计，**在三个尺度上进行预测**：

| 特征图大小 | 目标类型 | 来源层                             |
| ---------- | -------- | ---------------------------------- |
| 13×13      | 大目标   | 主干网络末端                       |
| 26×26      | 中目标   | 上采样 + 与浅层特征图拼接（1次）   |
| 52×52      | 小目标   | 上采样 + 与更浅层特征图拼接（2次） |

每个尺度都进行独立的边界框和类别预测。

---

#### 3. 每个尺度预测细节

- 每个尺度的每个 cell 使用 **3 个 anchor**（总共 9 个 anchor）。
- anchor 尺寸预先使用 K-means 聚类得到，分别分配给不同的尺度：
  - 大 anchor → 13×13
  - 中 anchor → 26×26
  - 小 anchor → 52×52

输出张量维度计算方式：

输出维度 = N × N × (3 × [4 + 1 + C]) = N × N × (3 × [bbox + obj_conf + classes])

---

#### 4. 分类预测方式变化（Softmax → Independent Logistic）

- YOLOv2 使用 Softmax 多分类（互斥假设，适用于单标签分类）。
- YOLOv3 改为 **每个类别独立的二分类 sigmoid + BCE loss（binary cross-entropy）**：
  - 更适合多标签分类任务（如 COCO，一个对象可以属于多个类别）。

**优点：**
- 更灵活，不再假设“只能属于一个类”
- 计算上更简单，不需要归一化

---

#### 5. 边界框预测方式（Anchor + Offset）

与 YOLOv2 相同，继续使用 anchor box 作为初始参考框，网络输出的是偏移值：

```text
tx, ty, tw, th → 通过以下方式计算真实坐标：
bx = sigmoid(tx) + cx
by = sigmoid(ty) + cy
bw = pw * exp(tw)
bh = ph * exp(th)
```

每个预测框还输出：

- objectness score（是否有目标）
- 每类的独立 sigmoid 分类概率

#### 6. 特征融合方式（FPN-like）

![image-20250508143018929](img\image-20250508143018929.png)

YOLOv3 的特征融合采用自上而下结构，与 FPN 类似：

- 将深层特征图 **上采样（nearest neighbor ×2）**
- 与中层或浅层的特征图 **按通道拼接**
- 实现跨层语义信息融合，提高小目标检测效果

#### 总结对比（YOLOv2 → YOLOv3）

| 项目           | YOLOv2                 | YOLOv3                                           |
| -------------- | ---------------------- | ------------------------------------------------ |
| 主干网络       | Darknet-19暗网-19      | **Darknet-53（残差 + 深层）**                    |
| 残差结构       | 无                     | **引入 ResNet-style 残差连接**                   |
| 检测尺度       | 单尺度（13×13）        | **三尺度（13×13, 26×26, 52×52）**                |
| 分类方式       | Softmax + MSE          | **独立 sigmoid + BCE loss** 哥sigmoid + BCE loss |
| 特征融合       | Passthrough            | **FPN-like 融合，上采样 + 拼接**                 |
| 框回归         | Anchor + offset锚+偏移 | Anchor + offset（相同）                          |
| 小目标检测能力 | 较差                   | **大幅提升**                                     |
| 速度           | 实时                   | 实时，略低于v2（但精度提升更大）                 |

## 📌V4版本

YOLOv4 是 YOLO 系列的重大更新版本，主打实用性和高性能，设计目标是 **不依赖特定硬件环境（如TPU）**，在常规GPU上即可实现SOTA性能。

---

####  相较 YOLOv3 的核心改进点（概览）

| 模块       | YOLOv3       | YOLOv4                          |
| ---------- | ------------ | ------------------------------- |
| 主干网络   | Darknet-53   | CSPDarknet-53                   |
| 数据增强   | 基础水平翻转 | Mosaic, CutMix, HSV扰动等       |
| 注意力机制 | 无           | SAM（Spatial Attention Module） |
| 特征融合   | FPN          | PAN（Path Aggregation Network） |
| 激活函数   | Leaky ReLU   | Mish                            |
| 正则化     | 无           | DropBlock, 标签平滑             |
| NMS        | 标准NMS      | DIOU-NMS                        |
| SPP        | 无           | 使用SPP进行多尺度增强           |
| 损失函数   | IOU/GIOU     | CIOU                            |

---

#### 1. 数据增强策略（Data Augmentation）

YOLOv4 采用了多种强大的数据增强方法，提高模型鲁棒性和泛化能力：

![image-20250508144721310](img\image-20250508144721310.png)

- **Mosaic Augmentation**：
  - 将 4 张图片拼接在一起组成一张图，提升小目标检测效果，增加背景复杂性。
- **CutMix / CutOut**：
  - CutMix：将一部分图像贴入另一张图中，同时调整标签。
  - CutOut：随机遮挡图像区域，提升对遮挡的鲁棒性。
- **HSV变换**：对图像进行亮度、饱和度、对比度扰动。
- Random Erase：用随机值或训练集的平均像素值替换图像的区域
- Hide and Seek：根据概率设置随机隐藏一些补丁
- **随机旋转/缩放/翻转**：提升方向泛化能力。

![image-20250508144938285](img\image-20250508144938285.png)

---

#### 2. DropBlock 正则化

- 类似于 Dropout，但针对 CNN 更有效。
- 特点：随机遮挡特征图的连续区域，抑制过拟合。
- 作用：使网络不能太依赖局部特征，提高泛化性能。

![image-20250508145022742](img\image-20250508145022742.png)

---

#### 3. 标签平滑（Label Smoothing）

- Soft label（如 0.9）替代硬标签（如 1.0），缓解过拟合，增强模型对异常样本的容忍度。
- 避免模型过度自信，有助于提升稳定性。

![image-20250508145050577](img\image-20250508145050577.png)

---

#### 4. 损失函数优化

问题：

YOLOv3 中使用的是 IOU / GIOU 损失，存在以下不足：
- IOU 在没有重叠时为0，无法提供梯度。
- GIOU 收敛速度慢，对长宽比敏感。

![image-20250508145140928](img\image-20250508145140928.png)

![image-20250508145238221](img\image-20250508145238221.png)

**CIOU（Complete-IOU）改进：**

- 综合考虑中心点距离、长宽比和重叠度。
- 公式考虑三个方面：
  1. IOU 本身。
  2. 框中心距离。
  3. 长宽一致性。
- **优势**：更快收敛、更稳健的定位性能。

---

#### 5. 改进的 NMS：DIOU-NMS

![image-20250508145615181](img\image-20250508145615181.png)

- 标准 NMS 只考虑 IOU。
- DIOU-NMS 在抑制重叠框时同时考虑框中心距离，优先保留中心靠近GT框的预测结果。
- 提高拥挤目标下的检测准确度。

---

#### 6. SPP（Spatial Pyramid Pooling）

- 在 backbone 与 neck 之间加入 SPP 层。
- 多尺度池化（1×1, 5×5, 9×9, 13×13）增加感受野。
- 保留空间信息同时聚合多尺度特征。

![image-20250508151123342](img\image-20250508151123342.png)

---

#### 7. SAM 模块（Spatial Attention Module）

- 空间注意力机制，提升关键区域的关注能力。
- 有助于突出显著目标、抑制背景干扰。
- 结构：基于卷积操作对空间位置赋予权重。

![image-20250508151225600](img\image-20250508151225600.png)

---

#### 8. PAN（Path Aggregation Network）

- YOLOv3 使用 FPN（自上而下的信息流）。

![image-20250508151431481](img\image-20250508151431481.png)

- PAN 增加自下而上的路径，加深信息流动，增强低层语义特征。

![image-20250508151515894](img\image-20250508151515894.png)

- 提升小目标检测与语义理解能力。

![image-20250508151538101](D:\wym\文档\notes\img\image-20250508151538101.png)

---

#### 9. 激活函数改进：Mish 激活函数

- YOLOv4 中大部分卷积层使用 **Mish** 激活。
- Mish 函数：`x * tanh(softplus(x))`，相较 ReLU 和 Leaky ReLU 更平滑。
- 好处：提供更强的非线性表示能力，提高精度和收敛速度。

![image-20250508151606978](img\image-20250508151606978.png)

---

#### 10. 主干网络结构：CSPDarknet53

- **CSPNet（Cross Stage Partial Net）** 结构引入，分割特征流一部分进入残差块，另一部分绕过。
- 优势：
  - 降低计算成本。
  - 减少冗余梯度。
  - 提升模型轻量化和表达能力。
  
  ![image-20250508151321505](img\image-20250508151321505.png)
  
  ![image-20250508152040506](img\image-20250508152040506.png)

---

#### 🔚 总结：YOLOv4 vs YOLOv3 对比表

| 模块       | YOLOv3     | YOLOv4              | 说明                       |
| ---------- | ---------- | ------------------- | -------------------------- |
| 主干网络   | Darknet-53 | CSPDarknet-53       | 更深更强大，优化梯度信息流 |
| 数据增强   | 翻转等基础 | Mosaic, CutMix, HSV | 极大提升鲁棒性和多样性     |
| 正则化     | 无         | DropBlock, 标签平滑 | 防过拟合                   |
| 损失函数   | GIOU       | CIOU                | 更快收敛，效果更优         |
| NMS        | 普通NMS    | DIOU-NMS            | 抑制误检更准确             |
| 多尺度感知 | 无         | SPP                 | 提升大/小目标识别能力      |
| 注意力机制 | 无         | SAM                 | 增强关键区域感知           |
| 特征融合   | FPN        | PAN                 | 加强语义与定位信息融合     |
| 激活函数   | Leaky ReLU | Mish                | 更高效的梯度传播           |
| 推理速度   | 快         | 稍慢                | 精度提升较大，适中牺牲速度 |

YOLOv4 以 YOLOv3 为基础，综合大量 CV 最新技巧，实现了检测性能的大幅提升。它在保持高推理效率的同时，通过引入 PAN、SAM、CSPDarknet、Mosaic 等模块，在小目标检测、定位精度、鲁棒性等方面有显著改进。

## 📌V7版本

### 🔧 网络结构优化

![image-20250514090732868](img\image-20250514090732868.png)

#### 	1.**RepVGG 推理加速（RepConv）**

- **训练时**使用多分支结构（例如 3×3 卷积 + 1×1 卷积 + BN），提升表达能力。

![image-20250514090820146](img\image-20250514090820146.png)

![image-20250514090849275](img\image-20250514090849275.png)

![image-20250514090912453](img\image-20250514090912453.png)

- **推理时**结构重参数化，将多分支折叠为单一 3×3 卷积，极大提高推理速度。

  ![image-20250514091015922](img\image-20250514091015922.png)

- 应用于 YOLO Head 与主干网络，兼顾性能与速度。

#### 	

####  2.**ResNet 残差链接转换**

- 将原始 ResNet 残差连接模块重构为适用于重参数化的结构，训练时保留残差，推理时合并路径。
- 保留梯度传递的优势，且便于推理优化。
- ![image-20250514091105688](img\image-20250514091105688.png)



---

### 📊 正样本分配策略优化

![image-20250514091226994](img\image-20250514091226994.png)

#### 3. **TaskAligned 正样本分配**
- 引入目标与分类的联合打分机制，避免传统基于 IOU 单一维度的样本分配缺陷。
- **公式**：

$$
score = (IOU^α) * (cls_score^β)
$$

- α、β 是超参数，控制 IOU 与分类置信度的权重。
- 提升样本质量，增强难样本的学习能力。

#### 4. **IOU 损失作为正样本选择依据**
- 将 IOU loss 作为一种“软标签”参与样本筛选。

- 与 GIoU、DIoU、CIoU 联动，提升定位精度。

  ![image-20250514091332898](img\image-20250514091332898.png)

​										![image-20250514091352879](img\image-20250514091352879.png)

![image-20250514091804350](img\image-20250514091804350.png)

![image-20250514091928335](img\image-20250514091928335.png)

![image-20250514092002241](img\image-20250514092002241.png)

### 🎯 多分支辅助训练机制

##### 5. **AUX 辅助输出（Auxiliary Head）**

![image-20250514092102018](img\image-20250514092102018.png)

- 在主干网络中加入辅助检测头（aux head）：
- 提供中间层的监督信号。
- 提升深层梯度传播能力，缓解深层网络训练难度。
- **训练时**启用，**推理时**剔除，不影响速度。

---

#### 6. **Model Scaling 策略改进**
- 提供统一缩放策略（depth、width、resolution）用于不同规模部署。
- 包含 YOLOv7-tiny、YOLOv7-W6、YOLOv7-E6 等多种模型尺寸。

#### 7. **协同训练策略**
- 吸收 Dense Prediction Transformer 的思想，引入深层监督、多路径融合。
- 模型更容易训练收敛，泛化能力增强。

### 📈 性能表现

- **准确率**超过 YOLOv4、YOLOv5、YOLOX 等同类模型。
- **推理速度**在 TensorRT 或 ONNX Runtime 下达到 SOTA 水平。
- **实际应用场景**中部署简易，适用于工业检测、无人驾驶、视频分析等高性能场景。

### 📝 总结

| 特性              | 优点                 | 备注             |
| ----------------- | -------------------- | ---------------- |
| RepConv           | 推理加速、表达能力强 | 重参数化技术核心 |
| ResNet结构重构    | 更高效残差连接       | 可转为轻量结构   |
| 正样本分配策略    | 更合理样本选择       | TaskAligned 分配 |
| IOU loss 分配依据 | 提升检测质量         | 与损失函数强结合 |
| AUX 辅助输出      | 深层训练稳定         | 推理无影响       |
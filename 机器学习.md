# 算法分类

## ![image-20230830175447247](img\image-20230830175447247.png)

### 机器学习建模流程

![image-20230831115439968](img\image-20230831115439968.png)



# KNN（K近邻）算法

KNN算法思想：如果一个样本在特征空间中的 k 个最相似的样本中的大多数属于某一个类别，则该样本也属于这个类别 

**样本相似性**：样本都是属于一个任务数据集的。样本距离(欧氏距离等)越近则越相似。

### K值的选择

![image-20230831152503338](img\image-20230831152503338.png)

### 特征归一化、标准化

特征的**单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级**，**容易影响（支配）目标结果**，使得一些模型（算法）无法学习到其它的特征。

#### 归一化

![image-20230831155813699](img\image-20230831155813699.png)

#### 标准化（大多选择）

通过对原始数据进行标准化，转换为均值为0标准差为1的标准正态分布的数据

![image-20230831160053298](img\image-20230831160053298.png)

#### 超参数选择的方法

交叉验证是一种数据集的分割方法，将训练集划分为 n 份，其中一份做验证集、其他n-1份做训练集集 。使用训练集+验证集多次评估模型，取平均值做交叉验证为模型得分

目的：得到更加准确可信的模型评分

#### 网格搜索

![image-20230831163559554](img\image-20230831163559554.png)

# 线性回归

线性回归(Linear regression)是利用 **回归方程(函数)** 对 **一个或多个自变量(特征值)和因变量(目标值)之间** 关系进行建模的一种分析方式。

1 为什么叫线性模型？因为求解的w，都是**w的零次幂**（常数项）所以叫成线性模型

2 在线性回归中，从数据中获取的规律其实就是学习权重系数w

3 某一个权重值w越大，说明这个权重的数据对预测结果影响越大 

### 损失函数

**误差概念**：用预测值y – 真实值y就是误差

**损失函数**：衡量每个样本预测值与真实值效果的函数

*均方误差* *(*Mean-Square Error, MSE*)*

*平均绝对误差* *(Mean Absolute Error* *,* *MAE)*

![image-20230901114816872](img\image-20230901114816872.png)

#### 梯度下降算法

求解函数极值更通用的方法就是梯度下降法。顾名思义:沿着梯度下降的方向求解极小值

梯度下降过程就和下山场景类似，可微分的损失函数，代表着一座山，寻找的函数的最小值，也就是山底

梯度: 导数（多元下为偏导）+斜率

![image-20230901183113930](img\image-20230901183113930.png)

#### 梯度下降过程

![image-20230901183152966](img\image-20230901183152966.png)

#### 梯度下降算法分类

全梯度下降算法FGD，使用全部数据集，训练速度慢。

随机梯度下降算法SGD，简单，高效，不稳定，每次只使用一个样本迭代，遇到噪声可能陷入局部最优解

mini-bantch 多数下使用，结合了前两者优点

SAG: 随机平均梯度下降算法，每轮更新都结合了上一轮梯度值

#### 正则方程和梯度下降算法对比

![image-20230901162716376](img\image-20230901162716376.png)

### 回归评估方法

平均绝对误差MAE，均方误差MSE，均方根误差RMSE （通过计算预测值和真实值做差求和后 平均/平方/平发再开根）

### 正则化

#### 模型拟合问题

- 拟合：用来表示模型对样本点的拟合情况

  - 欠拟合：模型在训练集上表现很差、在测试集表现也很差

    - 原因：模型过于简单，学习到数据的特征过少
    - 解决办法：添加其他特征项，添加多项式特征（二次，三次项增强泛化能力）
  - 过拟合：模型在训练集上表现很好、在测试集表现很差

    - 原因：模型太过于复杂、数据不纯、训练数据太少，存在一些嘈杂特征
    - 解决方法:重新清洗数据,增大数据训练量，正则化，减少特征维度
  - 泛化：模型在新数据集（非训练数据）上的表现好坏的能力
- 奥卡姆剃刀原则：给定两个具有相同泛化误差的模型，较简单的模型比较复杂的模型更可取

<img src="img\image-20230909170335989.png" alt="image-20230909170335989" style="zoom:50%;" />

**在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化**

#### L1正则化

假设𝐿(𝑊)是未加正则项的损失，𝜆是一个超参，控制正则化项的大小。用来进行特征选择，主要原因在于**L1正则化会使得较多的参数为0**，从而产生稀疏解,可以将0对应的特征遗弃，进而用来选择特征。一定程度上L1正则也可以防止模型过拟合。

#### **L2正则化（岭回归）**

假设𝐿(𝑊)是未加正则项的损失，𝜆是一个超参，控制正则化项的大小。主要用来防止模型过拟合，**可以减小特征的权重**。越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象

# 逻辑回归

### 逻辑回归概念 

• Logistic Regression，一种分类模型，把线性回归的输出，作为逻辑回归的输入。 

• 输出是(0, 1)之间的值

• 基本思想

1. 利用线性模型 f(x) = wx + b 根据特征的重要性计算出一个值
2. 再使用 sigmoid 函数将 f(x) 的输出值映射为概率值
   1. 设置阈值(eg:0.5)，输出概率值大于 0.5，则将未知样本输出为 1 类
   2. 否则输出为 0 类

3.逻辑回归的假设函数
 h(w) = sigmoid(wx + b )

线性回归的输出，作为逻辑回归的输入，主要解决二分类问题

### sigmoid函数

![image-20250305181721343](img\image-20250305181721343.png)

![image-20250305181749655](img\image-20250305181749655.png)

### 分类评估方法-混淆矩阵

![image-20250305182104425](img\image-20250305182104425.png)

        精确率也叫做查准率，指的是对正例样本的预测准确率(Precision) =TP/(TP+FP)
        召回率也叫做查全率，指的是预测为真正例样本占所有真实正例样本的比重 (Recall, TPR)= TP / (TP + FN)
    	准确率 (Accuracy) = (TP + TN) / 总样本数   
样本集中有 6 个恶性肿瘤样本，4 个良性肿瘤样本，我们假设恶性肿瘤为正例，则：

**模型 A：** 预测对了 3 个恶性肿瘤样本，4 个良性肿瘤样本

1. 真正例 TP 为：3 
2. 伪反例 FN 为：3
3. 假正例 FP 为：0
4. 真反例 TN：4
5. **精准率：3/(3+0) = 100%**
6. **召回率：3/(3+3)=50%**

如果我们对模型的精度、召回率都有要求，希望知道模型在这两个评估方向的综合预测能力如何？则可以使用 F1-score 指标。

![07](img\07.png)

### ROC曲线和AUC指标

ROC 曲线：我们分别考虑正负样本的情况：

1. 正样本中被预测为正样本的概率，即：TPR （True Positive Rate）
2. 负样本中被预测为正样本的概率，即：FPR （False Positive Rate）

![image-20230904182146483](img\image-20230904182146483.png)

ROC 曲线图像中，4 个特殊点的含义：

1. (0, 0) 表示所有的正样本都预测为错误，所有的负样本都预测正确
2. (1, 0) 表示所有的正样本都预测错误，所有的负样本都预测错误
3. (1, 1) 表示所有的正样本都预测正确，所有的负样本都预测错误
4. (0, 1) 表示所有的正样本都预测正确，所有的负样本都预测正确

### AUC 值

1. 我们发现：图像越靠近 (0,1) 点则模型对正负样本的辨别能力就越强
2. 我们发现：图像越靠近 (0, 1) 点则 ROC 曲线下面的面积就会越大
3. AUC 是 ROC 曲线下面的面积，该值越大，则模型的辨别能力就越强
4. AUC  范围在 [0, 1] 之间
5. 当 AUC= 1 时，该模型被认为是完美的分类器，但是几乎不存在完美分类器

> AUC 值主要评估模型对正例样本、负例样本的辨别能力.

# 决策树

决策树是一种树形结构，树中每个内部节点表示一个特征上的判断，每个分支代表一个判断结果的输出，每个叶子节点代表一种分类结果

相比其他学习模型，决策树在模型描述上有巨大的优势，决策树的逻辑推断非常直观，具有清晰的可解释性，也有很方便的模型的可视化。在决策树的使用中，无需考虑对数据量化和标准化，就能达到比较好的识别率

**决策树的建立过程**：

1.特征选择：选取有较强分类能力的特征。

2.决策树生成：根据选择的特征生成决策树。

3.决策树也易过拟合，采用剪枝的方法缓解过拟合。

## ID3决策树

ID3 树是基于信息增益构建的决策树.

**构建流程：**

1. 计算每个特征的信息增益
2. 使用信息增益最大的特征将数据集 S 拆分为子集
3. 使用该特征（信息增益最大的特征）作为决策树的一个节点
4. 使用剩余特征对子集重复上述（1，2，3）过程

### 信息熵

- 熵在信息论中代表随机变量不确定度的度量。

- 熵越大，数据的不确定性度越高

- 熵越小，数据的不确定性越低

  公式

  $$
  \large
  H = -\sum_{i=1}^{k}p_i\log(p_i)
  $$

### 信息增益

特征$A$对训练数据集D的信息增益$g(D,A)$，定义为集合$D$的熵$H(D)$与特征A给定条件下D的熵$H(D|A)$之差。即


$$
\large
g(D,A)=H(D)-H(D|A)
$$


根据信息增益选择特征方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，并选择信息增益最大的特征进行划分。**表示由于特征$A$而使得对数据D的分类不确定性减少的程度。**

![image-20230905115142450](img\image-20230905115142450.png)

![image-20230905115310935](img\image-20230905115310935.png)

## C4.5决策树

### 信息增益率

**信息增益比**本质： 是在信息增益的基础之上乘上一个惩罚参数。**特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大**。惩罚参数：数据集D以特征A作为随机变量的熵的倒数。

![image-20230905150018847](img\image-20230905150018847.png)

1. Gain_Ratio 表示信息增益率
2. IV 表示分裂信息、内在信息
3. 特征的信息增益 ➗ 内在信息
   1. 如果某个特征的特征值**种类较多**，则其内在信息值就越大。即：特征值**种类越多**，**除以**的**系数就越大**。
   2. 如果某个特征的特征值种类较小，则其内在信息值就越小。即：特征值种类越小，除以的系数就越小。

## Cart树简介

Cart模型是一种决策树模型，它即可以用于分类，也可以用于回归。

分类和回归树模型采用不同的最优化策略。Cart回归树使用平方误差最小化策略，Cart分类生成树采用的基尼指数最小化策略。

CART 回归树和 CART 分类树的不同之处在于:

1. CART 分类树预测输出的是一个离散值，CART 回归树预测输出的是一个连续值。
2. 分类树先计算所有特征基尼值和基尼指数，选择最小基尼指数特征作为分裂点
3. 回归树选择特征排序，计算均值作为代划分点，根据划分点分割数据集，两部分平方损失相加作为该点平方损失，取最小平方损失划分点并以此计算其它特征最优划分点，损失值。选出最小平方损失划分点作为树的分裂点
4. CART 分类树使用基尼指数作为划分、构建树的依据，CART 回归树使用平方损失。
5. 分类树使用叶子节点里出现更多次数的类别作为预测类别，回归树则采用叶子节点里均值作为预测输出

### 基尼指数

![image-20230905153217302](img\image-20230905153217302.png)





## 决策树对比

1. 信息增益（ID3）、信息增益率值越大（C4.5），则说明优先选择该特征。

2. 基尼指数值越小（cart），则说明优先选择该特征。

| **名称** | **提出时间** | **分支方式** | **特点**                                                     |
| -------- | ------------ | ------------ | ------------------------------------------------------------ |
| ID3      | 1975         | 信息增益     | 1.ID3只能对离散属性的数据集构成决策树  2.倾向于选择取值较多的属性 |
| C4.5     | 1993         | 信息增益率   | 1.缓解了ID3分支过程中总喜欢偏向选择值较多的属性  2.可处理连续数值型属性，也增加了对缺失值的处理方法  3.只适合于能够驻留于内存的数据集,大数据集无能为力 |
| CART     | 1984         | 基尼指数     | 1.可以进行分类和回归，可处理离散属性，也可以处理连续属性  2.采用基尼指数，计算量减小  3.一定是二叉树 |

## 决策树剪枝

> 剪枝 (pruning)是决策树学习算法对付 **过拟合** 的主要手段。
>
> 在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合

剪枝是指将一颗子树的子节点全部删掉，利用叶子节点替换子树(实质上是后剪枝技术)，也可以（假定当前对以root为根的子树进行剪枝）只保留根节点本身而删除所有的叶子

### 常见减枝方法

1. 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;

2. 后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。

   预剪枝

   - 优点：预剪枝使决策树的很多分支没有展开，不单降低了过拟合风险，还显著**减少了决策树的训练、测试时间开销**

   - 缺点：有些分支的当前划分虽不能提升泛化性能，甚至会导致泛化性能降低，但在其基础上进行的**后续划分却有可能导致性能的显著提高**

   - 缺点：预剪枝决策树也带来了**欠拟合的风险**

   后剪枝:

   - 优点：比预剪枝保留了更多的分支。一般情况下，后剪枝决策树的欠拟合风险很小，**泛化性能往往优于预剪枝**

   - 缺点:但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有非叶子节点进行逐一考察，因此在**训练时间开销**比未剪枝的决策树和预剪枝的决策树都要**大**得多。

# 集成学习

集成学习是机器学习中的一种思想，它通过多个模型的组合形成一个精度更高的模型，参与组合的模型成为弱学习器（基学习器）。训练时，使用训练集依次训练出这些弱学习器，对未知的样本进行预测时，使用这些弱学习器联合进行预测。

<img src="img\01.png" alt="01" style="zoom:67%;" />

## 集成学习算法分类

集成学习算法一般分为：bagging和boosting。

### bagging集成

Baggging 框架通过**有放回**的抽样产生不同的训练集，从而训练具有差异性的多个弱学习器，然后通过**平权投票、多数表决**的方式决定预测结果。

<img src="img\10.png" alt="10" style="zoom:67%;" />

### boosting集成

Boosting 体现了提升思想，每一个训练器重点关注前一个训练器不足的地方进行训练，通过加权投票的方式，得出预测结果。

Boosting是一组可将弱学习器升为强学习器算法。这类算法的工作机制类似：

1.先从初始训练集训练出一个基学习器

2.在根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到最大的关注。

3.然后基于调整后的样本分布来训练下一个基学习器；

4.如此重复进行，直至基学习器数目达到实现指定的值T为止。

5.再将这T个基学习器进行加权结合得到集成学习器。

**简而言之：每新加入一个弱学习器，整体能力就会得到提升**

<img src="img\11.png" alt="11" style="zoom:67%;" />

### Bagging和Boosting对比

**区别一:数据方面**

- Bagging：有放回采样
- Boosting：全部数据集, 重点关注前一个弱学习器不足

**区别二:投票方面**

- Bagging：平权投票
- Boosting：加权投票

**区别三:学习顺序**

- Bagging的学习是并行的，每个学习器没有依赖关系
- Boosting学习是串行，学习有先后顺序

## 随机森林

随机森林是基于 Bagging 思想实现的一种集成学习算法，它采用决策树模型作为每一个基学习器。其构造过程：

1. 训练：
   1. 有放回的产生训练样本
   2. 随机挑选 n 个特征（n 小于总特征数量）
2. 预测：平权投票，多数表决输出预测结果

<img src="img\image-20230905235722201.png" alt="image-20230905235722201" style="zoom: 50%;" />

说明：

（1）随机森林的方法即对训练样本进行了采样，又对特征进行了采样，充分保证了所构建的每个树之间的独立性，使得投票结果更准确。

（2）随机森林的随机性体现在每棵树的训练样本是随机的，树中每个节点的分裂属性也是随机选择的。有了这2个随机因素，即使每棵决策树没有进行剪枝，随机森林也不会产生过拟合的现象。

随机森林中有两个可控制参数：

- 森林中树的数量（一般选取值较大）
- 抽取的属性值m的大小。

**为什么要有放回地抽样？**

如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。

## adaboost算法

Adaptive Boosting(自适应提升)基于 Boosting思想实现的一种集成学习算法核心思想是通过逐步提高那些被前一步分类错误的样本的权重来训练一个强分类器。弱分类器的性能比随机猜测强就行，即可构造出一个非常准确的强分类器。其特点是：**训练时，样本具有权重，并且在训练过程中动态调整。被分错的样本的样本会加大权重，算法更加关注难分的样本。**

Adaboost自适应在于：“关注”被错分的样本，“器重”性能好的弱分类器:**（观察下图）**

 （1）不同的训练集--->调整样本权重

 （2）“关注”--->增加错分样本权重

 （3）“器重”--->好的分类器权重大

 （4） 样本权重间接影响分类器权重

<img src="img\boosting2.png" alt="boosting2" style="zoom:50%;" />

<img src="img\boosting6.png" alt="boosting6" style="zoom:50%;" />

<img src="img\boosting7.png" alt="boosting7" style="zoom:50%;" />

AdaBoost算法的两个核心步骤：

**权值调整：** AdaBoost算法提高那些被前一轮基分类器错误分类样本的权值，而降低那些被正确分类样本的权值。从而使得那些没有得到正确分类的样本，由于权值的加大而受到后一轮基分类器的更大关注。

**基分类器组合：** AdaBoost采用加权多数表决的方法。

- 分类误差率较小的弱分类器的权值大，在表决中起较大作用。

- 分类误差率较大的弱分类器的权值小，在表决中起较小作用。

<img src="img\image-20230906004228553.png" alt="image-20230906004228553" style="zoom: 80%;" />

## GBDT

梯度提升树（Gradient Boosting Decision Tre）是提升树（Boosting Decision Tree）的一种改进算法

梯度提升树不再使用拟合残差，而是利用最速下降的近似方法，利用**损失函数的负梯度**作为提升树算法中的残差近似值。

**GBDT 拟合的负梯度就是残差**，或者说对于**回归问题**，**拟合的目标值就是残差**

如果我们的 GBDT 进行的是**分类问题**，则损失函数变为 logloss，此时**拟合的目标值就是该损失函数的负梯度值**。

- GBDT算法流程

1 初始化弱学习器（目标值的均值作为预测值）

2 迭代构建学习器，每一个学习器拟合上一个学习器的负梯度

3 直到达到指定的学习器个数

4 当输入未知样本时，将所有弱学习器的输出结果组合起来作为强学习器的输出

### GBDT缺点

GBDT>预排序⽅法(pre-sorted)

 (1)空间消耗⼤。 

这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这⾥需要消耗训练数据两倍的内存。

 (2)时间上也有较⼤的开销。 在遍历每⼀个分割点的时候，都需要进⾏分裂增益的计算，消耗的代价⼤。

 (3)对内存(cache)优化不友好。 在预排序后，特征对梯度的访问是⼀种随机访问，并且不同的特征访问的顺序不⼀样，⽆法对cache进⾏优化。 同时，在每⼀层⻓树的时候，需要随机访问⼀个⾏索引到叶⼦索引的数组，并且不同特征访问的顺序也不⼀ 样，也会造成较⼤的cache miss

### GBDT和AdaBosst区别

 GBDT和其它Boosting算法⼀样，通过将表现⼀般的⼏个模型（通常是深度固定的决策树）组合在⼀起来集成⼀个 表现较好的模型。 

AdaBoost是通过提升错分数据点的权重来定位模型的不⾜，Gradient Boosting通过负梯度来识别问题，通过计算 负梯度来改进模型，即通过反复地选择⼀个指向负梯度⽅向的函数，该算法可被看做在函数空间⾥对⽬标函数进⾏优化。

## XGBOOST

全名叫极端梯度提升树，大多数的回归和分类问题上表现的十分顶尖

XGBoost 是对GBDT的改进：

1. 求解损失函数极值时使用泰勒二阶展开
2. 在损失函数中加入了正则化项（降低模型复杂度）
3. XGB 自创一个树节点分裂指标。这个分裂指标就是从损失函数推导出来的。XGB 分裂树时考虑到了树的复杂度。

构建最优模型的方法是**最小化训练数据的损失函数** 。

其过程如下：

1. 对树中的每个叶子结点尝试进行分裂
2. 计算分裂前 - 分裂后的分数：
   1. 如果gain > 0，则分裂之后树的损失更小，我们会考虑此次分裂
   2. 如果gain< 0，说明分裂后的分数比分裂前的分数大，此时不建议分裂
3. 当触发以下条件时停止分裂：
   1. 达到最大深度
   2. 叶子结点样本数量低于某个阈值

<img src="img\image-20230917143542540.png" alt="image-20230917143542540" style="zoom:50%;" />

### XGBOOST和GBDT的区别

- XGBoost⽣成CART树考虑了树的复杂度，GDBT未考虑，GDBT在树的剪枝步骤中考虑了树的复杂度<br/>
- XGBoost是拟合上⼀轮损失函数的⼆阶导展开，GDBT是拟合上⼀轮损失函数的⼀阶导展开，因此，XGBoost的准确性更⾼，且满⾜相同的训练效果，需要的迭代次数更少<br/>
- XGBoost与GDBT都是逐次迭代来提⾼模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进⾏，⼤⼤提⾼了运⾏速度

## lightGBM

### ![image-20250306173507217](img\image-20250306173507217.png)

> 常⽤的机器学习算法，例如神经⽹络等算法，都可以以mini-batch的⽅式训练，训练数据的⼤⼩不会受到内存限制。

> ⽽GBDT在每⼀次迭代的时候，都需要遍历整个训练数据多次。

> 如果把整个训练数据装进内存则会限制训练数据的⼤⼩；如果不装进内存，反复地读写训练数据⼜会消耗⾮常⼤的时 间。

> LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地⽤于⼯业实践。

lightGBM主要基于以下⽅⾯优化，提升整体特特性： 

1. 基于Histogram（直⽅图）的决策树算法 
2. Lightgbm的Histogram（直⽅图）做差加速 ，根据直方图的离散值，遍历寻找最优的分割点
3. 带深度限制的Leaf-wise的叶⼦⽣⻓策略
4. 分箱的方式来处理特征值，减少参与模型训练的样本数量
   1. 根据样本 梯度 来对 梯度小的样本（单边）进行 采样 ，而对梯度大的样本保留
   2. 分箱操作会使IV变小，变量间相关性变大

5. 基于EFB，减少参与建树的特征
   1. 从特征的角度出发， 降低数据的维度。EFB 互斥特征捆绑

6. 带深度限制的Leaf-wise的叶子生长策略
7. 直接⽀持类别特征
8. 直接⽀持⾼效并

**基于Histogram（直⽅图）的决策树算法**

先把连续的浮点特征值离散化成k个整数，同时构造⼀个宽度为k的直⽅图。 在遍历数据的时候，根据离散化后的值作为索引在直⽅图中累积统计量，当遍历⼀次数据后，直⽅图累积了需要的 统计量，然后根据直⽅图的离散值，遍历寻找最优的分割点。

![image-20250306174448570](img\image-20250306174448570.png)

使⽤直⽅图算法有很多优点。⾸先，最明显就是内存消耗的降低，直⽅图算法不仅不需要额外存储预排序的结果，⽽且 可以只保存特征离散化后的值，⽽这个值⼀般⽤8位整型存储就⾜够了，内存消耗可以降低为原来的1/8

![image-20250306174502195](img\image-20250306174502195.png)

后在计算上的代价也⼤幅降低，预排序算法每遍历⼀个特征值就需要计算⼀次分裂的增益，⽽直⽅图算法只需要计算 k次（k可以认为是常数），时间复杂度从O(#data#feature)优化到O(k#features)

由于特征被离散化后，找到的并**不是很精确的分割点**，所以会对**结果产⽣影响**。 但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响**并不是很⼤**，甚⾄有时候会更好⼀点。原因是**决策 树本来就是弱模型**，分割点是不是精确并不是太重要；**较粗的分割点也有正则化的效果，可以有效地防⽌过拟合**；即使 单棵树的训练误差⽐精确分割的算法稍⼤，但在梯度提升（Gradient	Boosting）的框架下没有太⼤的影响

**Lightgbm的Histogram（直⽅图）做差加速**

⼀个叶⼦的直⽅图可以由它的⽗亲节点的直⽅图与它兄弟的直⽅图做差得到.

通常构造直⽅图，需要遍历该叶⼦上的所有数据，但直⽅图做差仅需遍历直⽅图的k个桶

LightGBM可以在构造⼀个叶⼦的直⽅图后，可以⽤⾮常微⼩的代价得到它兄弟叶⼦的直⽅图，在速度 上可以提升⼀倍

![image-20250306174857853](img\image-20250306174857853.png)

**带深度限制的Leaf-wise的叶⼦⽣⻓策略**

Level-wise便利⼀次数据可以同时分裂同⼀层的叶⼦，容易进⾏多线程优化，也好控制模型复杂度，不容易过拟合

但实际上Level-wise是⼀种低效的算法，因为它不加区分的对待同⼀层的叶⼦，带来了很多没必要的开销，因为实 际上很多叶⼦的分裂增益较低，没必要进⾏搜索和分裂。

Leaf-wise则是⼀种更为⾼效的策略，每次从当前所有叶⼦中，找到分裂增益最⼤的⼀个叶⼦，然后分裂，如此循环。 因此同Level-wise相⽐，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。 Leaf-wise的缺点是可能会⻓出⽐较深的决策树，产⽣过拟合。因此LightGBM在Leaf-wise之上增加了⼀个最⼤深度 的限制，在保证⾼效率的同时防⽌过拟合

**直接⽀持类别特征**

可以直接输⼊类别特 征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相⽐0/1展开的⽅法，训练速度可以加速8倍，并且精度⼀致。

**直接⽀持⾼效并⾏**

LightGBM原⽣⽀持并⾏学习，⽬前⽀持特征并⾏和数据并⾏的两种。

-  **特征并⾏**的主要思想是在**不同机器在不同的特征集合上分别寻找最优的分割点**，然后在机器间同步最优的分割点

  - 在特征并⾏算法中，通过在本地保存全部数据避免对数据切分结果的通信

  ![image-20250306175742371](img\image-20250306175742371.png)

-  **数据并⾏**则是让不同的机器先在**本地构造直⽅图**，然后进⾏全局的合并，最后在**合并的直⽅图上⾯寻找最优分割 点**

  - 在数据并⾏中使⽤分散规约把直⽅图合并的任务分摊到不同的机器，降低通信和计算，并利⽤直 ⽅图做差，进⼀步减少了⼀半的通信量
  - 基于投票的数据并⾏(Voting Parallelization)则进⼀步优化数据并⾏中的通信代价，使通信代价变成常数级别。在 数据量很⼤的时候，使⽤投票并⾏可以得到⾮常好的加速效果

![image-20250306175730480](img\image-20250306175730480.png)

![image-20250306175805688](img\image-20250306175805688.png)

> StratifiedKFold 分层划分, 保证每一组数据中, 不同类别标签占比和原始数据比例一致

> splitter.split(X,y) 返回的是数据的索引

> LGB的API  早停 early stopping:

> 设置了这个参数之后, 在训练模型的时候指定的验证集, 验证集的指标连续 early stopping 参数指定的轮数 不再有提升, 训练就会结束, 不一定会训练出 满足 n_estimators 指定的弱学习器来

> 设置这个参数的目的就是避免过拟合

# 朴素贝叶斯

**条件概率：** 表示事件A在另外一个事件B已经发生条件下的发生概率，P(A|B)

**联合概率：** 表示多个条件同时成立的概率，P(AB)   =  P(A)   P(B|A)    

特征条件独立性假设：P(AB) = P(A) P(B)

## 贝叶斯公式

![image-20250306180419927](img\image-20250306180419927.png)

1. P(C) 表示 C 出现的概率
2. P(W|C) 表示 C 条件 W 出现的概率
3. P(W) 表示 W 出现的概率

### 朴素贝叶斯

朴素贝叶斯在贝叶斯基础上增加：**特征条件独立假设**，即：特征之间是互为独立的。

### 拉普拉斯平滑系数   

为了避免概率值为 0，我们在分子和分母分别加上一个数值，这就是拉普拉斯平滑系数的作用。

### 朴素⻉叶斯优缺点

优点：

- 朴素⻉叶斯模型发源于古典数学理论，有稳定的分类效率
- 对缺失数据不太敏感，算法也⽐较简单，常⽤于⽂本分类

- 分类准确度⾼，速度快

缺点：

- 由于使⽤了样本属性独⽴性的假设，所以如果特征属性有关联时其效果不好<br/>

- 需要计算先验概率，⽽先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设<br/>

  的先验模型的原因导致预测效果不佳<br/>

**朴素⻉叶斯是⽣成模型**
根据已有样本进⾏⻉叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进⽽求出联合分布概率P(XY),最后利⽤⻉叶斯定理求解P(Y|X)，<br/>
⽽**LR是判别模型**，
根据极⼤化对数似然函数直接求出条件概率P(Y|X)；朴素⻉叶斯是基于很强的条件独⽴假设（在已知分类Y的条件下，各个特征变量取值是相互独⽴的），⽽LR则对此没有要求；

朴素⻉叶斯适⽤于数据集少的情景，⽽LR适⽤于⼤规模数据集。

## 特征降维

用于训练的数据集特征对模型的性能有着极其重要的作用。如果训练数据中包含一些不重要的特征，可能导致模型的泛化性能不佳。例如：

1.  某些特征的取值较为接近，其包含的信息较少
2.  我们希望特征独立存在，对预测产生影响，具有相关性的特征可能并不会给模型带来更多的信息，但是并不是说相关性完全无用。

**降维** 是指在某些限定条件下，**降低特征个数**， 我们接下来介绍集中特征降维的方法：

低方差过滤法，相关系数法，PCA（主成分分析）降维法。

1. **低方差过滤法** 指的是删除方差低于某些阈值的一些特征。
2. 相关系数法:皮尔逊相关系数、斯皮尔曼相关系数。特征之间的相关系数法可以反映变量之间相关关系密切程度
3. **主成分分析**（PCA）PCA 通过对数据维数进行压缩，尽可能降低原数据的维数（复杂度），损失少量信息，在此过程中可能会舍弃原有数据、创造新的变量



# 聚类算法

一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。

在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。

![image-20250306181623419](img\image-20250306181623419.png)

## k-means聚类流程

1、随机设置K个特征空间内的点作为初始的聚类中心

2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别

3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）

4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程

<img src="img\image-20230907103735327.png" alt="image-20230907103735327" style="zoom:50%;" />

## 评价指标

**SSE-误差平方和**

![image-20250306181917675](img\image-20250306181917675.png)

**SC系数**

![image-20250306182012456](img\image-20250306182012456.png)

**肘部法**

SSE 变化过程中会出现一个拐点，下降率突然变缓时即认为是最佳 n_clusters 值。

在决定什么时候停止练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别。![image-20250306182100237](img\image-20250306182100237.png)

CH 系数

CH 系数结合了聚类的凝聚度（Cohesion）和分离度（Separation）、质心的个数，希望用最少的簇进行聚类。

# 支持向量机

**SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。**

![image-20250306182248099](img\image-20250306182248099.png)

球—— **「data」数据**

棍子—— **「classifier」分类**

最大间隙——**「optimization」最优化**

拍桌子——**「kernelling」核方法**

纸——**「hyperplane」超平面**

案例来源：http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/

### SVM的定义

**SVM全称是supported vector machine（支持向量机），即寻找到一个超平面使样本分成两类，并且间隔最大。**

SVM能够执行线性或非线性分类、回归，甚至是异常值检测任务。是机器学习领域最受欢迎的模型之一。SVM特别适用于中小型复杂数据集的分类。

**超平面最大间隔**

![image-20250306182501963](img\image-20250306182501963.png)

**硬间隔**

如果样本线性可分，在所有样本分类都正确的情况下，寻找最大间隔，这就是硬间隔

**软间隔和惩罚系数**

允许部分样本，在最大间隔之内，甚至在错误的一边，寻找最大间隔，这就是软间隔

目标是尽可能在保持间隔宽阔和限制间隔违例之间找到良好的平衡。

通过惩罚系数C来控制这个平衡：C值越小，则间隔越宽，但是间隔违例也会越多。

### **核函数**

核函数，是将原始输入空间映射到新的特征空间，从而，使得原本线性不可分的样本可能在核空间可分

![image-20250306182705895](img\image-20250306182705895.png)

![image-20250306182824241](img\image-20250306182824241.png)

高斯核函数使用较多。

1. 线性核：一般是不增加数据维度，而是预先计算内积，提高速度
2. 多项式核：一般是通过增加多项式特征，提升数据维度，并计算内积
3. 高斯核（RBF、径向基函数）：一般是通过将样本投射到无限维空间，使得原来不可分的数据变得可分。

任务:找到一种方法，用一条线将数据完美分类。如果只从1维的角度考虑，这是一项不可能完成的任务，但可以用升维度的办法来解决问题。

让我们引入一个函数 f(x)，图像如下图所示。 将 x 的每个值映射到其对应的输出。使得所有蓝点在Y轴的输出更大，而红点在Y轴的输出偏小。此时，我们可以使用一条水平线将数据完美分类。

![image-20250306183026634](img\image-20250306183026634.png)

从上面的结果中看出，当gamma=0.1的时候，结果已经接近线性SVC了。此时模型已经欠拟合了。自此我们可以得出结论

- gamma越大，模型过拟合风险越高
- gamma越小，模型欠拟合风险越高

# 金融风控项目涵盖知识点

## 整体流程

### 金融风控背景评分卡模型

- 欺诈风险
- 信用风险 评分卡解决的是信用风险
  - A/B/C   A卡最重要的,  不见得每个公司都有C卡
  - A 新客(在当前业务观察点之前, 没有还款表现)
  - B 未逾期的老客( 在观察点时刻, 不能处于逾期的状态)
  - C 逾期的老客 (在观察点的时刻处于逾期的状态)

### 建模流程

- 明确做什么业务 (熟悉能用到哪些数据, 样本设计)

- 风控场景下样本设计

  - DPD  0 , 1,  灰样本(只参与模型评估, 不参与模型训练)
  - 观察点 观察期 表现期
    - 如何确定表现期   通过滚动率确定 1标签的阈值, 通过1标签的阈值 结合vintage分析 得到业务的表现期
    - 观察期用于做特征, 表现期用于做标签, 之所以要注意这一点, 就是因为上线之后, 获取不到观察点之后的数据(未来的数据)
  - 滚动率 vintage报表

### 特征工程 

> 需要及时跑出一个base line **基线模型**, 做一段特征之后, 就要跑一下, 跟基线模型做比较

- 明确取数逻辑, 从哪张表获取数据都要有文档确定下来
- 取数 → 探索性分析(覆盖度, 单特征**KS**/AUC) → 数据清洗
- 特征衍生
- 特征筛选
- 特征分箱/编码  **卡方分箱/WOE变换**
- 特征单调性 bin_var 图
  - 特征分箱, 分箱之后计算每一箱的违约率, 绘制成柱状(每一箱占比)折线(每一箱的违约率)图

### 模型训练

- 超参数选择  交叉验证, 网格搜索  随机搜索
- 集成学习超参数调整
  - 随机森林 bagging   maxDepth 可以不用做限制
  - GBDT boosting  maxDepth 
    - 每个基学习器 树的最大深度不要过大, 过大了基本会过拟合
    - 学习率  n_estimator

### 评估方法

- AUC 通用指标
- KS  风控场景下用到的指标

- 风控业务场景需要算模型报告 , 要知道模型报告的作用
  - 模型开发阶段, 可以对比两个模型的区分度和排序能力
  - 模型上线阶段, 通过模型报告可以对模型的稳定性进行监控

- 实际做业务的时候, 需要做很多监控的报表

  - 特征监控报表
    - 缺失率
    - 区分度变化  稳定性
      - 分箱统计每一箱的违约率
  - 模型监控报表
    - 区分度的变化 稳定性

## 机器学习模型做数据挖掘套路

- 逻辑回归
- 集成学习
- 特征衍生套路
  - 用户ID 分组聚合 (天猫复购, 捷信风控数据)
  - 时序数据处理 (把数据整理成时序状态, 按照时序数据处理套路)
  - 特征交叉(GBDT(LGB)+LR)
- 特征筛选套路
  - 缺失
  - 相关 corr vif(实际筛选的时候, 算一轮只去掉一个特征)
    - 两个特征相关性较强, 去掉区分度比较差的
  - 区分 (风控场景 IV)
    - 单特征AUC KS  如果所有的特征单特征AUC 都是0.5附近的
    - 特征重要性
    - 递归特征消除
  - 稳定性 (不见得所有的业务场景都会用)
    - PSI  特征和时间相关性是不是很强 风控场景 
- 分箱/编码套路
  - 分箱好处 稳定/对异常值容忍度比较高/方便处理缺失值/引入非线性
    - 等频/等距/决策树/KMeans
  - 编码
    - one-hot

## 模型效果增强的方法

- 异常值处理 LOF/IForest
  可以考虑使用IForest 清洗掉训练集中的噪声(异常比较大的数据) 可以提升模型的泛化性能

- 样本不均衡问题处理

  SMOTE 过采样 (预测误差较大的数据, 先去掉再过采样)

- 缺失值的处理

  如果特征比较重要, 尽量保留

  如果是类别型的特征可以考虑添加一个特征的取值, 缺失 

  也可以考虑用业务默认值来填充

  在特征衍生的过程中出现了缺失值, 可以考虑结合业务做缺失值处理

  - XGB/LGB 可以不处理缺失值

## 金融风控常见指标

MOB 第几期账单

DPD 逾期天数  DPD1 逾期1天

M1/M2  逾期情况用月份来表示

- M1 逾期30天以内
- M2  逾期30~60天

## 建模流程概述

### ABC评分卡

- 贷前 申请评分卡 **Application score card**   一些规模比较小的公司, 只有A卡
  - 数据都是新客

- 贷中 行为评分卡 Behavior score card 
  - 老客 

- 贷后 催收评分卡 Collection score card   很多公司没有C卡

### 建模前准备

- Y标签定义问题

  - 阈值(违约多少天可以作为1标签)  
    -  分客群(正常还款, M1, M2 , M3, M4.)来观察滚动率 M0→ M1的比例 , M1→ M2的比例
    -  如果违约的继续劣化的比例超过了60% 可以考虑作为1标签的阈值

  - 表现期 (从哪个时间点的老客中选取样本进行模型训练)
    - 通过vintage分析  经过了N期账单  满足阈值条件的客群 比例基本没有太大的上升, 这个账单的时间就可以作为表现期
    - 表现期只能做标签
    - 观察期  观察点 表现期  , 观察期用来做特征, 一般至少6个月

  - 灰样本:  DPD90+ 1标签   DPD5以下是0标签   DPD5~DPD90 是灰样本
    - 灰样本不参与模型训练, 但是参与模型评估
    - 模型输出的灰样本的违约概率, 应该在0样本和1样本之间

  - 样本量
    - 最少 1500 个0标签 1500个1标签
  - 时间外样本 

### 概念相关

- ABC评分卡  最重要的是A卡, 都是二分类问题
  - A/B/C卡建模用到的客群有区别
- 使用老客的数据训练模型,  有新的数据进来可以对未来的情况进行预测
- 特征构造/特征衍生的套路
  - 当一个用户不止一条数据的时候, 很重要的方法就是分组聚合
  - 使用用户ID 分组
    - 按照R/F/M的思路进行聚合
    - 数值型计算统计量(最大值, 最小值, 方差, 均值,非0值的个数, 计数, 求和, 极差(最大值-最小值))
    - 类别型 统计不同取值的数量
- 特征工程的思路
  - 通过特征衍生/特征构造/特征交叉 先把所有可能想到的特征构造方式都做出来 再通过特征筛选的套路, 把和目标值相关性比较强的特征留下

## 规则挖掘

当一个用户有多条记录的时候, 可以使用分组聚合的方式

- 数值类型的数据, 计算各种统计量, 在衍生的过程中不必考虑统计方式和目标值之间的关联, 最后通过特征筛选在挑出关联性比较强的, 如果特征真的进到模型, 需要解释这种聚合的方式对用户是否违约有什么样的相关性
- 风控业务中, 对可解释性是有很强要求的, 特别是做国内的业务, 每一步都要具备可解释性
  - 特征衍生, 模型的结果都要具备可解释性

规则挖掘可以使用决策树

### 未来信息

特征穿越/标签泄露/未来信息指的都是一回事儿，在做特征的时候, 使用了来自表现期中的数据

可能导致出现未来信息的原因：缺少日志表/快照表   添加日志表/快照表，选取了观察点之后的数据

### 时序数据特征衍生

时序特征聚合处理的原因:  发现用户负债/资产状况随时间的变化情况

时序特征处理: 按时间序列做聚合处理，进行无差别衍生处理，通过筛选套路选出最有用的特征

构造特征要从两个维度看数据：归纳+演绎

- 归纳：从大量数据的结果总结出规律（相关关系） 从数据中只能得出相关性
- 演绎：从假设推导出必然的结果（因果关系）

### 特征变换

> 分箱把连续型的变量变成类别型

- 分箱（离散化）后的特征对异常数据有很强的鲁棒性
- 单变量分箱（离散化）为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力
- 分箱（离散化）后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
- 分箱（离散化）后，模型会更稳定，如对年龄离散化，20-30为一个区间，不会因为年龄+1就变成一个新的特征
- 特征离散化以后，可以将缺失作为独立的一类带入模型

分箱方式

等频，等距，卡方分享

- 在等频/等距的基础上, 计算卡方值, 合并卡方值较小的箱体, 目的是为了构造出, 每一箱的违约率都与均值差距较大的情况

分箱后可对类别型数据做woe编码变换

什么时候做分箱和编码

- 特征衍生的时候产生的分组聚合的结果, 好多都是数值型, 在模型训练之间基本都要走分箱和编码
- 如果是使用无差别的套路是的聚合衍生方式, 可以先算一算相关性/单特征AUC 先过滤掉区分度很低的, 再做分箱和编码
- 分箱编码调整了之后, 特征的区分度也会随之变化, 如果换了分箱的阈值, 编码之后还要算一下区分度

### 特征筛选

#### 单特征筛选

从几个角度衡量：覆盖度，区分度，相关性，稳定性

- 覆盖度高(缺失率低, 零值率低)
- 区分度高(IV 高, 单特征AUC 0.6以上)
  - IV < 0.02: 区分度小，IV [0.02, 0.5]: 区分度大，IV > 0.5: 作为单独规则使用
  - 区分度  AUC(0.75)  KS = max(TPR-FPR)
    - AUC 反应了模型的整体表现
    - KS 反应的模型区分度最好的一点
- 稳定性强(PSI 比较小)
  - PSI<0.1  没有重大变化，PSI 0.1-0.25 有变化，需关注，PSI >0.25  有重大变化
- 相关性, 特征之间彼此相关性不要太大

使用toad 来做单特征筛选, 从iv, 缺失率, 相关性, 三个维度,一次性筛选出满足条件的好特征来

```python
#缺失率 传入0.5 缺失率高于0.5的会被删除  iv 0.05   corr 两两计算相关性, 相关性高于0.7的特征组, 会去掉iv比较小的特征
selected_data, drop_list = toad.selection.select(data,target='creditability',empty=0.5,iv=0.05,corr=0.7,return_drop=True)
```

#### 多特征筛选

**使用集成学习的特征重要性**

- 集成学习模型都可以输出特征重要性 feature_importances_
- feature_importances_ 是一个相对的值, 不同的模型输出的结果不尽相同, 我们只能够通过 feature_importances_获取特征的相对重要性
  - 使用特征重要性做特征筛选, 很重要的就是要确定阈值, 星座特征/Boruta都可以帮助我们来找到要去掉的特征(找阈值)
  - 特征重要性低于星座特征的可以删掉(把星座作为特征放入集成学习模型参与训练)
  - Boruta 会帮助我们在原始特征的基础上, 通过打乱特征值的顺序创建一些假的影子特征, 如果影子特征的重要性高于原始特征, 说明这个特征也可以删除

**方差膨胀系数 VIF**

- corr 两两相关
- VIF 一个特征是否能被其它特征线性表示

vif=1/(1-R*R) 

- R²<0.5 → 弱拟合
- 0.5 ≤ R² ≤ 0.8 → 中度拟合
- R² > 0.8 强拟合

当R²越大，这个特征越能被其它特征线性表示, 当VIF超过某个阈值的时候，可以考虑将这个特征删除

R²代表了预测值和真实值拟合的拟合程度，既考虑了预测值与真实值的差异，同时也兼顾了真实值的离散程度

R² 只能代表拟合的强弱，但不能说R² 越大模型越好，它只是意味着实际点与拟合点的平均偏差很小。

![image-20250324165203856](img\image-20250324165203856.png)

**RFE递归特征消除**

用排除法的方式训练模型，把模型性能下降最少的那个特征去掉，反复上述训练直到达到指定的特征个数

### 补：方差和偏差

**(1) 偏差（Bias）**

- **定义**：模型预测值的期望与真实值之间的差异。
- **本质**：模型对数据内在规律的**错误假设**导致的系统性误差。
- 表现：
  - 高偏差：模型过于简单（如线性模型拟合非线性数据），无法捕捉数据特征，导致**欠拟合**。
  - 低偏差：模型复杂，能够更好地拟合数据。

**(2) 方差（Variance）**

- **定义**：模型对训练数据中随机噪声的敏感程度，即模型在不同训练集上的预测结果的波动性。
- **本质**：模型对训练数据中**偶然性噪声的过度学习**。
- 表现：
  - 高方差：模型过于复杂（如高阶多项式回归），对训练数据中的噪声敏感，导致**过拟合**。
  - 低方差：模型对数据变化不敏感，泛化能力较强。

**. 偏差-方差权衡**

- **核心矛盾**：降低偏差通常会增加方差，反之亦然。
- **目标**：通过调整模型复杂度，找到偏差和方差的**平衡点**，使总误差最小。

### 特征监控

内部特征监控：覆盖度/缺失率，区分度，AUC，KS，IV

外部特征评估: 覆盖度/区分度/稳定性  覆盖度：对方数据能够名中的样本数量/自己客群的样本数量

### 评分卡构建

训练逻辑回归模型->计算fpr，tpr->计算ks->绘制ROC曲线

使用lightGBM训练模型->获取特征重要性

关于特征重要性, 需要注意的是, 这是一个相对的结果, 重要性得分的大小只在当前次训练有效果。不能在不同的模型对比得分， 当模型参数发生了变化、训练的数据发生变化， 重要性得分也会有变化

- 可以通过相对的结果判断哪个特征相对更加重要
- 可以结合着具体模型表现， 决定要去掉/保留哪些特征, 也可以换几组数据, 多跑几次这个结果, 计算重要性的平均分, 获取一个更加可靠的重要性排序

集成学习两类模型, 调参需要注意的地方

- Bagging 随机森林 通过投票可以解决方差大的问题
  - 随机森林的基学习器,树的最大深度没必要做特殊限制
- Boosting 解决的是偏差问题
  - 基学习器尽量不要搞的过于复杂 maxdepth 3-5

### LightGBM 评分卡

集成学习的特征重要性的大小跟数据/参数 相关性比较强, 我们可以通过交叉验证的方式, 多划分几个不同的数据集, 算出多次特征重要性, 计算平均值 , 这个平均的特征重要性, 可信度更高

特征重要性结果是否可信, 还要看模型效果

- auc >0.75  KS 不要太小  0.3以上

## toad流程

- 加载数据，探索性数据分析，同时处理数值型和字符型，整理开发样本、验证样本与时间外样本  
-  特征筛选(缺失值,IV,相关系数)
  - 若后续建模过程要对变量进行分箱处理，该操作会使变量的IV变小，变量间的相关性变大，因此此处可以对IV和相关系的阈值限制适当放松
- 卡方分箱，并绘制Bivar图，调整分箱（调整趋势）
- 绘制负样本占比关联图
  - 若图中的线没有交叉,故不需要对该特征的分组进行合并。如有错位，不再容忍范围可合并
- WOE编码,并验证IV
  - 计算训练样本与测试样本的PSI，删除大于0.13psi的特征
  - 通常单个特征的PSI值建议在0.1以下，根据具体情况可以适当调整
- 使用的IV和相关系数阈值进行特征筛选
  - 因为卡方分箱后的IV降低，相关程度增大，再次筛选特征
- 使用逐步回归特征筛选，选择ks为评价指标
  - **Forward selection** ：将自变量逐个引入模型，引入一个自变量后查看该模型是否发生显著性变化
  - **Backward elimination** ：与Forward selection选择相反，将所有变量放入模型，尝试将某一变量进行剔除，查看剔除后对整个模型是否有显著性变化
  - **both** ：将前向选择与后向消除同时进行
- 定义模型并训练
  - 分别进行正向调用和逆向调用
  - 如逆向模型训练集KS值明显小于正向模型训练集KS值，说明当前时间外样本分布与开发样本差异较大，需要重新划分样本集。(样本量较小时经常发生)
- 绘制AUC曲线模型，
  - XGBoost模型的效果并没有明显高于逻辑回归模型，因此当前特征不需要再进行组合。
  - 逆向调用LR模型的训练集结果，没有显著好于正向调用的时间外样本结果，该模型在当前特征空间下几乎没有更多的优化空间
  - 正向LR模型的结果训练集KS值，与时间外样本KS值的差值在5%以内，故不需要调整跨时间稳定性较差的变量
- 计算指标评估模型,生成模型报告 分别计算 F1-score KS和AUC
- 生成评分卡（利用ScoreCard函数重新训练并生成评分卡）

### 样本不均衡问题的处理

加权

- class_weight = balanced
- 原理 样本占比比较少的类别 乘上一个比较大的权重, 样本占比比较多的类别 乘上一个比较小的权重
  - 所有类别样本乘上不同权重之后, 实现平衡的结果

过采样/下采样

- SMOTE, 采用了K近邻的思路, 每个1样本(占比较少的类别), 找到它最近的K个邻居, 在这个样本和邻居之间进行插值, 填上一个假样本
  - 过采样之前, 需要将噪声样本去掉
    - 模型预测和实际情况差距较大的样本去掉, 不参与采样的过程 , 这个思路来自于LGB (基于梯度的单边采样)
  - 过采样的数据只能用于模型的训练, 不用这个数据来计算评估的指标

### 异常点检测

异常点检测的无监督算法的应用场景

- 数据清洗, 把异常程度较高的样本去掉, 可以提高模型的泛化能力, 降低模型的过拟合程度
- 评分卡场景: 
  - 利用无监督的异常检测算法, 做数据的初筛, 利用免费的数据训练无监督模型, 异常程度较高的样本不再调用付费数据, 直接拒绝
  - 在冷启动阶段, 没有1标签的情况下, 可以使用异常检测算法做无监督的申请评分卡, 把异常程度较高的用户直接拒绝

LOF

- 基于密度的思路, 异常点所在的区域密度小, 正常点所在的区域密度大
  - LRD 点a的lrd，首先计算a到它的所有k个最近邻居的reach-dist(可以看做就是两点之间的距离)，并取该数字的平均值，lrd是该平均值的倒数
  - LRD告诉我们，从一点到另一个点或者另一堆点, LRD越小，密度越低，距离越远

- LOF 局部异常因子（local outlier factor）：将每个点的lrd与它们的k个邻居的lrd相比较
  - 某点的LOF= K个邻居的LRD的平均值/该点的LRD
  - LRD 越小 密度越低距离越远 ，离群点的LRD小，它的邻居的LRD会比较大
  - 离群点的LOF = 较大的邻居的LRD平均值/ 较小的离群点的LRD >>1

IForest原理

- 将数据样本随机采样,随机选择一个特征, 做二叉划分, 最终划到分无可分的情况(每个叶子结点只有一个样本, 每个叶子结点的样本特征值都一样)
  - 如果样本的异常程度比较高, 在孤立森林中, 所处的树的平均深度会比较浅 
  - 如果样本的异常程度比较低, 在孤立森林中, 所处的树的平均深度会比较深

## 拒绝推断

> 风控建模中，幸存者偏差是普遍存在的问题
>
> 被拒绝的样本是模型预测分数较低的人群。缺少该部分低分人群的信息，对全局样本表示模型的影响非常大。当模型经过多次迭代后，其重要特征可能被逐渐弱化，因此需要使用相应手段进行处理

拒绝推断要解决的问题

- 使用各种手段, 让被拒绝的用户, 也能填上一个标签(1/0 违约/没有违约)

  - 硬截断法

    - 直接使用KGB模型在拒绝样本上做预测，并将低分样本（如分数最低的20%样本）认为是负样本，带入模型进行估计，其余拒绝样本全部视为灰色样本，不予考虑
    - 精度较为敏感的风控系统中，硬截断法显然是不合理

  - 模糊展开法

    - 模糊展开法将每条拒绝样本复制为不同类别、不同权重的两条
    - 假设当前有一个拒绝样本，KGB模型预测其为负样本的概率为0.8，为正样本的概率为0.2，则分别生成两条新样本。第一个样本标签为负（'bad_ind'=1），权重为0.8；第二个样本标签为正（'bad_ind'=0），权重为0.2。将两条样本分别带入AGB模型进行训练
    - 模糊展开法通过权重调整，修正模型的偏差，其效果与AGB模型的识别能力相关性较高

  - 重新加权法

    - 重新加权法不使用拒绝样本进行学习，而仅利用其样本分布特点，调整原KGB数据集分布权重。
    - 基于逾期概率升序，等频分箱求负样本占比，将负样本乘当前箱的权重修正项
    - 将权重带入建模过程，得到新的模型

    ![image-20250324185428928](img\image-20250324185428928.png)

  - 迭代再分法

    - 通过多次迭代，保证模型结果有效且收敛的拒绝推断方法
    - 先使用硬截断法为拒绝样本的标签赋值
    - 将具有“伪标签”的样本加入原KGB模型进行学习，得到部分标签失真的AGB模型
    - 使用AGB模型获取拒绝样本的逾期概率。之后再次使用硬截断法
    - 不断重复上述过程，直至某个指标收敛

## 可解释性 SHAP

- 内在可解释（Intrinsic Interpretability）指的是模型自身结构比较简单，使用者可以清晰地看到模型的内部结构，模型的结果带有解释的效果，如逻辑回归，线性回归，决策树
- 事后可解释（Post-hoc Interpretability）指的是模型训练完之后，使用一定的方法增强模型的可解释性，挖掘模型学习到的信息。有的模型自身结构比较复杂，使用者很难从模型内部知道结果的推理过程，模型的结果也不带有解释的语言，通常只是给出预测值，这时候模型是不具备可解释性的。事后可解释是指在模型训练完之后，通过不同的事后解析方法提升模型的可解释性。
- **局部解释**指的是当一个样本或一组样本的输入值发生变化时，解释其预测结果会发生怎样的变化。例如，在银行风控系统中，我们需要找到违规的客户具备哪个或哪些特征，找到潜在的违规客户；当账户金额发生变化时，违规的概率会如何变化；在拒绝了客户的信用卡申请后，可以根据模型的局部解释，向这些客户解释拒绝的理由
- **全局解释**指的是整个模型从输入到输出之间的解释，从全局解释中，我们可以得到普遍规律或统计推断，理解每个特征对模型的影响

**SHAP**

> SHAP（SHApley Additive exPlanations）是一种用博弈论方法（起源于合作博弈论）来解释机器学习模型输出的方法,SHAP通过计算模型中各个特征的边际贡献来衡量各个特征的影响大小，进而对黑盒模型进行解释。

- SHAP是一个理论完备的解释方法，即完整的博弈理论。其中的对称性、可加性、有效性等公理使得该解释变得更加合理。
- 其次，SHAP方法公平地分配了**样本中每个特征的贡献值**，最终解释了单个样本模型预测值与平均模型预测值之间的差异。
- 最后，Shapley Value可以有不同的对比解释，其既可以解释单个样本的模型预测值与平均模型预测值之间的差异，也可以解释单个样本的模型预测值与另一个样本的模型预测值之间的差异。
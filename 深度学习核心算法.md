# 深度学习核心算法

## 神经网络基础

> 神经网络的本质任务是将人类眼中的数据（如图像、文本等）转换成计算机能识别的矩阵、数值或特征

###  线性函数

1. ##### 基本概念

   - 线性回归: 预测一个标签y，y与数据中的多个特征x1, x2等有关系，通过引入参数w1, w2等来表示这些关系。
   - 逻辑回归: 不预测具体值，而是预测一个分类。
   - 神经网络: **多层的线性回归**，通过多层线性回归逐步得到最终结果，具有**强可拓展性**。

2. 矩阵表示

   - 数据表示: 在任务中面对的不是单个数值而是**矩阵**，**输入**图像、**权重参数**、**偏置**以及**输出**结果都是矩阵形式
   - 权重参数: 对应每个输入特征，也需要有相应的权重参数矩阵。

3. 多类别分类任务

   - 十分类任务: 预测输入属于各个类别的概率，而不是直接预测类别ID。
   - 多组权重参数矩阵: 对于十分类任务，需要十组权重参数矩阵，每组矩阵对应一个类别的得分。
   - **偏置: 对结果进行微调**，偏置的个数与想要得到的结果类别数相同，也是十个。

4. 计算方法

   - 矩阵乘法：通过权重参数矩阵W与输入特征矩阵x的乘法，得到各类别的得分
   - 加偏置: 在得分基础上加上偏置进行微调。

### 权重初始化

1. 基本概念：权重初始化是指在神经网络训练开始前，为网络中的权重参数赋予初始值的过程，随机生成
2. 权重初始化对结果的影响：初始权重过大或过小都可能导致模型训练效果不佳
3. **选择原则**：选择较小的初始权重，避免一开始因权重过大导致模型过拟合或不佳，在训练中逐步探索
4. **独立性**：每个特征点所对应的权重参数是独立的，权重参数也是分别进行更新的，w1、w2、w3等权重参数在更新时互不影响

### 损失函数

> 凭借预测得分高低不能全面评估模型性能，利用损失函数来量化模型预测结果与期望结果之间的差异

1. 概念:损失函数是神经网络中用于**衡量模型预测结果与真实结果之间差异的函数**。它是神经网络优化过程中的关键组成部分，通过**最小化损失函数**，我们可以使**模型逐渐逼近最优解**。

2. 损失函数的计算:在分类任务中，我们可能会比较正确类别的预测得分与其他类别的得分；在回归任务中，则可能直接计算预测值与真实值之间的差值。

3. 损失函数与模型优化的关系:通过不断调整权重参数来最小化损失函数，从而使模型的预测结果更加接近真实结果

4. 正则化惩罚：希望模型不要太复杂，过拟合模型是没用的

   ![企业微信截图_17434752956646](img\企业微信截图_17434752956646.png)

正则化与损失函数平衡：

矛盾点：正则化惩罚希望模型不要过于复杂，而损失函数期望预测值和真实值尽可能接近

平衡方法:引入正则化参数λ，作为系数来平衡预测准确性和模型复杂度。

总结: 正则化是**防止模型过拟合，提升泛化能力的重要手段**。通过加入正则化惩罚项，可以限制模型复杂度，使模型在保持预测准确性的同时，更加简洁和通用。

###  Softmax分类器

> 将得分值转换为概率值，引入非线性

<img src="img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-10" alt="img" style="zoom:50%;" />

作用: 将任**意实数输入映射到(0,1)之间**，且所有**输出值的和为1**，便于**进行概率解释**。

<img src="img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-11" alt="img" style="zoom:50%;" />

应用: 在多分类任务中，通过Softmax分类器将模型的输出得分转换为概率分布，从而选择概率最大的类别作为预测结果。

<img src="img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-12" alt="img" style="zoom:50%;" />

优点: 能够处理多分类问题，且输出具有概率意义，便于理解和比较不同类别之间的相对可能性。

计算流程：

1. 得分计算:对每个类别计算得分值
2. 指数映射:对得分值进行指数映射
3. 归一化:将指数映射的结果进行归一化得到概率值
4. 损失计算:使用交叉熵损失函数计算预测概率与真实标签之间的差异

注意:

- 指数映射能够拉开得分值之间的差异，使得模型对差异更加敏感。
- 归一化步骤确保所有输出值的和为1，满足概率分布的要求。



### 前向传播

> 前向传播是神经网络从输入层到输出层逐层计算预测值的过程。数据通过权重矩阵、偏置向量和激活函数的组合，最终得到预测结果。

**步骤**：

1. **输入层**：接收原始输入数据 `X`。
2. **线性变换**：每一层计算加权和 `Z = W·A_prev + b`  
   - `W`：权重矩阵  
   - `A_prev`：前一层输出（输入层为 `X`）  
   - `b`：偏置向量  
3. **激活函数**：应用非线性激活函数（如ReLU、Sigmoid）  `A = σ(Z)`  
4. **逐层传递**：重复上述步骤，直到输出层得到预测值 `y_pred`。

**作用**

- 生成预测结果。
- 为反向传播提供中间计算结果（如 `Z`, `A`）。

---

### 反向传播

> 反向传播通过**链式法则**计算损失函数对模型参数的梯度，从输出层向输入层逐层更新权重和偏置。

**步骤**

1. **计算损失**：根据预测值 `y_pred` 和真实值 `y_true` 计算损失 `L`（如MSE、交叉熵）。
2. **输出层梯度**：计算损失对输出层输入的梯度 `dZ_output = dL/dy_pred * σ'(Z_output)`。
3. **链式求导**：逐层反向计算梯度：  
   - 权重梯度：`dW = (dZ)·A_prev.T / m`  
   - 偏置梯度：`db = sum(dZ, axis=1) / m`  
   - 前一层梯度：`dA_prev = W.T·dZ`  
   - 激活梯度：`dZ_prev = dA_prev * σ'(Z_prev)`  （`m` 为样本数）
4. **更新参数**：使用优化器（如SGD、Adam）根据梯度更新权重和偏置：  
   `W = W - η·dW`  
   `b = b - η·db`  
   （`η` 为学习率）

**作用**

- 逐步计算参数梯度，指导模型优化方向(梯度下降最快)。
- 通过最小化损失函数，提升模型预测精度。

核心思想:梯度下降，通过迭代逐步优化权重，每次更新一小步。

---

### 前向反向传播对比

| **特性**       | **前向传播**                 | **反向传播**                 |
| -------------- | ---------------------------- | ---------------------------- |
| **方向**       | 输入层 → 输出层              | 输出层 → 输入层              |
| **主要计算**   | 线性变换 + 激活函数          | 链式求导 + 梯度计算          |
| **目的**       | 生成预测结果                 | 优化模型参数                 |
| **存储中间值** | 保存 `Z` 和 `A` 用于反向传播 | 复用前向传播的中间值计算梯度 |

---

 **整体流程**

1. 前向传播计算预测值。
2. 计算损失函数。
3. 反向传播计算梯度。
4. 使用优化器更新参数。
5. 重复迭代直到收敛。

### 神经网络结构

![img](img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-15)

- 层次结构: 神经网络由输入层、隐藏层、输出层组成，形成层次化的特征提取过程。
- 神经元: 每个圆圈代表一个神经元，可视为一个特征提取器。
- 全连接: 每一层的神经元与下一层的所有神经元相连，实现信息的全面传递。
- 非线性: 引入激活函数等非线性元素，使网络能够拟合更复杂的函数关系。

![image-20250401110719792](img\image-20250401110719792.png)

1. 神经元：

   - **输入层**: 相当于人类认知的特征，如身高指标或图像像素点，文本字词
   - **隐层**:将人类认知特征转换为计算机理解的数据特征
   - **不可解释性**:神经网络的输出值（特征值）是通过一系列复杂的计算得到的，这些计算过程中的系数（权重）和计算方式（如加权和）往往无法直接解释其具体的含义或来源。

2. 权重参数的学习与更新:

   - **权重参数**: 在神经网络中，用于**将输入层的特征转换成隐层特征值的系数称为权重参数**。
   - 学习与更新: 权重参数不是随机的，而是通过模型的学习过程来确定的。模型会根据预测结果与实际标签的差距（损失）来调整权重参数，使得预测结果越来越接近实际标签。
   - 目标: **权重参数的学习与更新的目标是使得神经网络能够更好地拟合数据**，即预测结果与实际标签的差距最小化。
   - 过程: 在学习过程中，模型会尝试不同的权重参数组合，**通过计算损失来评估这些组合的好坏，并选择损失最小的组合作为最终的权重参数**。

3. 全连接:

   - 神经网络的层次思想:神经网络通过层级的思想，将输入数据的特征进行**多次变换和聚合**。每一步变换都相当于对特征的一次重新组合，通过多个隐层逐步**提取出更深层次的特征**
   - 特征聚合:**每一层隐层都会对前一层的输出进行再组合**，形成新的特征表示。这种逐层聚合的方式有助于提取出更加复杂和抽象的特征
   - 学习本质:学习如何提取和表示输入数据的特征
   - 特征提取:自动地从输入数据中提取出对任务有用的特征,对于后续的分类、检测、识别等任务至关重要
   - 学习能力:神经网络的学习能力体**现在其能够不断地调整权重参数**

4. 非线性:

   - 线性操作与替换问题:神经网络的每一层都可以看作是一个线性操作,(wx+b)如果将多层神经网络的权重参数矩阵连续相乘，得到一个总的权重参数矩阵，那么神经网络就退化成了一个简单的线性模型,因此引入激活函数，使得每一层的输出都是非线性的，从而能够学习到更加复杂的特征表示
   - 非线性激活: 非线性激活函数是神经网络中不可或缺的部分。它们**为神经网络引入了非线性因素，使得模型能够学习到输入数据的复杂模式和规律**。常见的非线性激活函数包括**ReLU**、Sigmoid、Tanh等。
   - 线性模型的区分能力较弱，难以复杂地划分数据，非线性有更强的区分能力

5. 激活函数:

   - Sigmoid函数,Sigmoid函数在早期神经网络中广泛使用，但存在梯度消失问题，当输入值较大或较小时，**梯度**接近于0(梯度消失)，导致网络难以更新
   - ReLU函数:ReLU函数因其通用性和高效性，在大多数情况下是首选。

   ![img](img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-17)

神经网络结构回顾

- 层次结构: 神经网络包括输入层、隐藏层和输出层，其中隐藏层可以有多层。
- 全连接: 每一层的神经元都与上一层的所有神经元相连。
- 非线性: 通过激活函数引入非线性，增强网络的表达能力。

Dropout技术,每一层随机丢弃一些神经元

![img](img\p-db574365a9450ebeeedb025207f0d1f4-40-2025032100-24)

### 知识小结

| 知识点           | 核心内容                         | 考试重点/易混淆点            | 难度系数 |
| ---------------- | -------------------------------- | ---------------------------- | -------- |
| 神经网络基础     | 神经网络结构、前向传播、反向传播 | 神经网络与传统机器学习区别   | 中       |
| 激活函数         | Sigmoid、ReLU等                  | ReLU与Sigmoid的优缺点        | 中       |
| 损失函数         | 交叉熵损失、均方误差等           | 不同任务中的损失函数选择     | 中       |
| 正则化           | L1、L2正则化                     | 正则化参数对模型的影响       | 中       |
| 优化算法         | 梯度下降法                       | 学习率的选择与调整           | 中       |
| 神经元与层次结构 | 输入层、隐藏层、输出层           | 层次结构对模型复杂度的影响   | 易       |
| 过拟合与欠拟合   | 训练集与验证集的表现             | 防止过拟合的方法             | 中       |
| Dropout          | 随机丢弃神经元                   | Dropout率的选择              | 中       |
| 批量归一化       | 数据标准化                       | 批量归一化对训练稳定性的影响 | 难       |



## 卷积神经网络CNN

> 卷积神经⽹络是⼀种特殊的神经⽹络，⼴泛应⽤于图像识别、物体检测等领域,还可以处理视频中的时间维度，逐帧处理。

### 卷积神经网络与传统神经网络区别

**传统神经网络特点:**

- 结构: 传统神经⽹络通常由输⼊层、隐藏层和输出层组成，每层之间的神经元全连接
- 数据处理: 适⽤于结构化数据
- 计算⽅式: 使⽤wx + b的⽅式进⾏计算，也称为全连接层（FC）或多层感知机（MLP）

**传统神经⽹络处理图像数据的局限性:**

- 参数量⼤: **图像数据通常具有⾼维度，**如300x300像素的图像，每个像素有3个颜⾊通道，则输⼊层有27万个神经元。若隐藏层有128个神经元，则仅第⼀层就有27万 ×128个参数，**参数量巨⼤**
- 不适合图像特征提取: 传统神经⽹络**难以有效提取图像中的空间特征**，因为图像中的像素之间具有**空间关系**，⽽全连接层忽略了这种关系

**卷积神经⽹络处理图像数据的优势:**

- 参数量少: 卷积神经⽹络通过**卷积层、池化层**等结构，⼤⼤**减少了参数量**，使得⽹络更加轻便。
- 适合图像特征提取: 卷积层能够**提取图像中的局部特征**，并通过**多层卷积**逐渐提取更**⾼级的特征**，⾮常适合处理图像数据。
- 空间关系保留: 卷积神经⽹络能够**保留图像中的空间关系**，使得⽹络能够更好地理解图像内容

**卷积神经⽹络的⽬标:**

- 核⼼⽬标: 卷积神经⽹络的核⼼⽬标是通过新颖的计算⽅式，如卷积操作，来减少参数量，提⾼⽹络的计算效率和泛化能⼒。
-  实现⽅式: 通过**卷积层、池化层、全连接层**等结构的组合，以及**参数共享、稀疏连接等技巧，实现参数量的减少**

### 卷积神经网络架构

- 输⼊层: 接收**原始图像数据**。
- 卷积层: 对输⼊图像进⾏**卷积操作，提取特征**。
- 池化层: 对卷积层输出的特征图进⾏**下采样**，**降低特征图的维度**。
- 全连接层: 将池化层**输出的特征图展平成⼀维向量**，并通过**全连接⽹络进⾏分类或回归**。
-  输出层: **输出最终的分类结果或回归值**，通常使⽤**Softmax激活函数进⾏多分类**。

![image-20250401144744760](img\image-20250401144744760.png)

### 卷积神经网络的优势

- **局部连接**: 卷积层中的每个神经元只与输⼊图像的局部区域相连，减少了参数数量
- **权值共享**: 卷积核在图像上滑动时，权值保持不变，进⼀步减少了参数数量
- **平移不变性**: 卷积神经⽹络对图像的平移具有⼀定的鲁棒性（模型在面对‌**异常输入、干扰、不确定性或意外条件**‌时，仍能保持稳定运行或维持预期性能的能力。），能够识别图像中的平移不变特征
- **层次化特征学习**: 通过多层卷积和池化操作，卷积神经⽹络能够逐层学习图像的层次化特征

### 卷积核与特征图

**卷积核的作用：**

- 卷积核定义:卷积核(filter)在图像处理中用于提取特定区域的特征
- 区域交流: 卷积核通过与输入图像的特定区域进行深入交流，挖掘该区域的特征信息
- 卷积核大小: 常见卷积核3*3核5*5,3*3计算效率高，比较常用
- 硬件优化:3*3卷积核在硬件上计算效率最**高**

**特征图的生成:**

![image-20250401145653924](img\image-20250401145653924.png)

- 特征图定义: 特征图（feature map）是通过卷积核在输⼊图像上滑动并计算得到的特征点组合。
- 多特征图: 使⽤多个不同的卷积核可以提取多种不同的特征，⽣成多个特征图，增加特征的多样性。
- 卷积核与通道: 卷积核的通道数必须与输⼊图像的通道数相匹配，例如RGB图像有三个通道，卷积核也应有三个通道

**特征提取多样性:**

- 多卷积核优势: 使⽤多个卷积核可以提取更多样化的特征，提⾼模型的表达能⼒
- 特征选择: 每个卷积核专注于提取某种特定类型的特征，如颜⾊、边缘等
- ⽆对错之分: 不同卷积核提取的特征没有绝对的好坏之分，多样性是关键

### 卷积的相关概念

卷积出发点：图像中的**相邻像素点**之间存在**关系**，卷积通过**窗⼝学习**这些关系，使模型能够理解图像中的局部特征

卷积目的: 为了降低计算量，我们将图像**切分成多个区域**，**每个区域计算⼀个特征**，从⽽**减少计算量并更⼴泛地考虑图像信息**

**卷积的操作过程**:

- 区域划分：输⼊**图像切分**成多个⼩区域
- 特征计算：对于每个**⼩区域，计算⼀个特征值**，是对该区域内像素点信息的综合反映
- 卷积核：卷积过程中使⽤的**窗⼝或滤波器**，⽤于提取图像中的特定特征
- 卷积操作：**卷积核在图像上滑动**，对每个⼩区域进⾏特征计算，最终得到特征图。

**卷积计算过程:**

- 内积计算：卷积核与输⼊图像对应区域的计算实质上是**内积运算**，即对应位置**相乘后求和**
- 偏置项: 在内积计算的结果上加上**偏置项**，得到最终的特征值
- 参数共享：卷积核的**权重在滑动过程中是共享**的，即在同⼀输⼊图像上，卷积核的权重保持不变。
- 滑动步长: 卷积核在输入图像上滑动的**步长影响特征图的尺寸核特征提取的细致程度**，步长越小，特征取值越细致

![image-20250401145934161](img\image-20250401145934161.png)

**卷积操作中的参数：**

1. 步长选择：

   - 步⻓推荐值: 在常规任务中，**步⻓通常设为1**，因为1是最⼩的步⻓，能够确保遍历整个输⼊区域
   - 步⻓与遍历区域的关系: **步⻓增⼤时，遍历的区域会减⼩**，导致**提取的特征也减少**

2. 卷积核的个数:

   - 卷积核个数的意义: 卷积核的个数决定了能提取到的特征数量,每个卷积核都能得到⼀组特定的特征图
   - 卷积核个数的选择:理论上，卷积核个数越多越好,因为能获取更多信息,常见:256,512。

3. 卷积核大小:在同⼀组卷积（同⼀层）中，所有**卷积核的⼤⼩**规格必须⼀致，如都是3 × 3或5 × 5（效率高，见卷积核的作用）

4. 卷积操作的公平性:

   - 不公平性的表现：卷积操作对输⼊数据的**不同位置并不是公平的**，**边缘区域的值被卷积的次数较少，⽽中⼼区域的值被卷积的次数较多**
   - 不公平性的影响: 边缘区域的值对最终特征的影响较⼩，⽽**中⼼区域的值对最终特征的影响较⼤**
   - 解决⽅法: 引⼊**padding（填充）来平衡边缘和中⼼区域被卷积的次数**

5. 边界填充:

   - 定义:原始图像或特征图的周围添加额外的边界,改变卷积操作的结果
   - 目的:使得边缘像素点不再位于边缘，更充分地参与卷积运算
   - 填充值选择:通常为0，只占位，不引入额外特征信息，避免引入不必要的噪声或特征，计算也简单

   ![image-20250401152629571](img\image-20250401152629571.png)

**卷积操作的层次性**:

​		卷积操作在神经⽹络中通常是层次进⾏的，即⼀层卷积完成后，再进⾏下⼀层卷积， 

​		通过多层次的卷积操作,**逐步提取更高层次的特征信息**,增强神经网络的表达能力和泛化能力

**卷积层数与感受野：**

1. 多层卷积的必要性

   - 多层卷积有助于提取图像中不同层次的特征
   - 举例: 第⼀个3x3卷积关注图像中的细节特征（如⻓颈⿅的斑点），第⼆个3x3卷积在第⼀个卷积的基础上组合特征，关注更⼤区域（如⻓颈⿅的花纹组合），第三个卷积则进⼀步组合成更全局的特征

2. 感受野的概念:

   - **感受野**是指在卷积神经⽹络中，**某⼀层输出特征图上的某个像素点所对应的输⼊图像上的区域⼤⼩**

   - 随着卷积层数的增加，感受野逐渐增⼤，意味着⽹络能够捕捉到更全局的信息

   - ⽐喻: 浅层⽹络（如平⺠⽼百姓）关注局部、细节信息；中层⽹络（如村⻓）考虑更⼴泛的信息（如村庄规划）；深层⽹络（如市⻓）则思考更全局、

     ⻓远的问题（如城市规划、未来发展）

3. 深度学习中**深度**的含义

   - 指的是**网络层数的增加，导致感受野的增大和信息汇总程度的提高**
   - 深层网络能捕捉更高级，更全局的特征

4. 卷积层数的选择：

   - 卷积层的选择根据具体任务而定，深层网络适合大目标检测，但可能无法有效检测小目标
   - 不同网络结构有不同的设计，不同场景下没有固定的最佳层数，根据任务需求**灵活调整**
   - 理论卷积层越多，提取的特征越丰富，但需要平衡计算资源和任务需求，也可能导致过拟合问题

![image-20250401171442228](img\image-20250401171442228.png)

![image-20250401171946875](img\image-20250401171946875.png)

![image-20250401172153411](img\image-20250401172153411.png)

### 池化层的定义与作用

>池化层是神经⽹络中的⼀种层，⽤于对特征进⾏筛选，保留重要特征，去除不重要的特征。

- 减少特征数量，奖励计算复杂度
- 提取代表性特征，提高模型泛化能力
- 通过逐步减小特征的空间大小，实现特征的浓缩
- 池化层通常也被称为下采样层

### 池化层的类型与选择

1. 最⼤池化: 选择每个位置上的最⼤特征值作为该位置的输出。
2. 其他池化⽅式: 除了最⼤池化，还有平均池化等⽅式，根据具体任务选择合适的池化
   ⽅式。
3. 平均池化是⼀种将多个数值取平均值的过程
4. 池化层⼤⼩的选择: 池化层的⼤⼩（如2 × 2）和步⻓（如2）可以根据任务需求进⾏调
   整，以实现不同程度的特征筛选和浓缩
5. 池化层的有点与必要性:
   - 减少特征的数量，降低计算复杂度,提⾼计算速度,
   - 提取最具代表性的特征，增强模型的泛化能⼒
   - 去除不必要的特征，减少过拟合风险
6. 平均池化的应用场景：
   - 全局向量获取：获取某个特征图或某个区域的全局向量
   - 输出层前的整合: 神经网络的输出层之前整合不同特征图的向量

### 网络结构中的层

**⽹络结构的层**: 在任务中，⽹络结构通常包括输⼊层、卷积层、激活函数层、池化层等。这些层组合在⼀起，形成了⼀种循环或重复的结构，即block。每个block内部可能包含多个卷积、⾮线性操作和池化等操作

**block**:可以看成一个大块，卷积块，内部执行了许多操作，这种块状结构让网络设计更加模块化和灵活

**卷积的重复性**: 卷积和其他结构在⽹络中通常是以重复block的形式出现的。这种重复性使得⽹络能够⾼效地处理数据，同时也有助于⽹络的拓展和修改

### 全连接层

> 特征图虽然包含了数据的特征信息，但并不能直接⽤于分类.将特征图拉⻓成⼀个⼀维向量,接着，通过全连接层,对拉⻓后的向量进⾏处理，输出各个类别的概率,计算可以表示为WX + b.W是权重矩阵，X是拉⻓后的特征向量，b是偏置项

作用:全连接层（FC）主要负责整合卷积层提取的特征，并输出最终的结果或预测值。

位置:全连接层通常位于神经⽹络的末端，紧跟在卷积层、池化层等特征提取层之后

神经网络层数的计算:按照带有权重参数的层来计算,卷积层、全连接层等带有权重参数的层被计⼊总层数,⽽池化层等不带有权重参数的层则不计⼊

### 批量标准化BN

**卷积神经⽹络流⾏的原因**: 卷积神经⽹络之所以能够在各种应⽤场景中取得良好的效果，很⼤程度上得益于其强⼤的特征提取能⼒和灵活的⽹络结构。这使得卷积神经⽹络能够跨领域、跨专业地进⾏学科交叉，解决各种复杂的问题。
**BN技术**: BN（Batch Normalization）技术是卷积神经⽹络中的⼀个重要组成部分。它通过在每个⼩批量数据上进⾏归⼀化操作，使得数据的分布更加稳定，从⽽加速了神经⽹络的训练过程，提⾼了模型的收敛速度和性能。BN技术可以说是卷积神经⽹络流⾏的⼀个重要推⼿，没有它，卷积神经⽹络的效果可能会⼤打折扣。

**批量标准化原理**：BN通常与卷积绑定使用，BN相当于在卷积之后再次进⾏标准化操作，它会对特征进⾏微调，使离群的特征点向中⼼靠拢，但不影响⼤部分正常特征

**作用:**调整数据分布，使特征更加稳定,⽅便下⼀层继续学习。

**可学习性**:BN层并不是简单地应⽤⼀个固定的标准化公式，⽽是根据数据的特点和分布进⾏适当程度的拉回操作,动态调整标准化的程度,这种学习是自动的

### 知识小结

| 知识点              | 核心内容                     | 考试重点/易混淆点                     | 难度系数 |
| ------------------- | ---------------------------- | ------------------------------------- | -------- |
| 卷积神经网络（CNN） | 图像和视频处理               | 卷积操作与全连接层的区别              | 中       |
|                     | 行为识别、姿态估计、追踪任务 | 卷积核大小的选择（如3x3）             | 中       |
|                     | 3D点云数据处理               | 卷积层与池化层的配合                  | 高       |
| 卷积层              | 特征提取                     | 卷积核参数共享                        | 中       |
|                     | 内积计算                     | 步长（stride）与填充（padding）的影响 | 中       |
| 池化层              | 特征筛选（最大池化）         | 池化层与下采样的关系                  | 低       |
| 批归一化（BN）      | 特征分布稳定                 | BN层的位置与作用                      | 中       |
| 经典网络结构        | AlexNet、VGG、ResNet         | 各网络结构的特点与改进点              | 高       |
| 残差网络（ResNet）  | 解决深度网络退化问题         | 同等映射的理解                        | 高       |

## 循环神经网络与Transformer

### 递归神经网络RNN

1. 递归神经网络的概念
   - 定义:递归神经网络（RNN）是一种用于**处理序列数据的神经网络**，特别适用于时间序列和文本任务。
   - 特点：RNN能够处理具有**时序依赖性**的数据，即当前时刻的输出与之前的输入有关
   - 领域:  语音识别、自然语言处理、时间序列预测
2. RNN与卷积神经网络的区别
   - 任务类型: RNN主要用于处理序列数据，如时间序列和文本；而卷积神经网络（CNN）则更擅长处理图像数据。
   - 网络结构: RNN具有循环结构，能够捕捉序列中的时序依赖性；CNN则通过卷积层和池化层来提取图像特征。
3. RNN基本原理：
   - 循环结构: RNN通过**隐藏层中的循环连接来捕捉序列中的时序信息**，使得网络能够“记住”之前的输入
   - 时序依赖性: RNN的当前**输出不仅与当前输入有关，还与之前的输入和隐藏状态有关**，从而实现了对序列数据的建模。
   - 参数共享: 在RNN中，不同时间步的**参数是共享**的，这大大**减少了网络的参数数量**，提高了训练效率
4. RNN的局限性及改进：
   - 局限性: RNN在处理长序列时可能会遇到**梯度消失或梯度爆炸**的问题，导致**无法有效捕捉长距离依赖关系**
   - 改进方法: 为了解决这些问题，人们提出了多种改进方法，如长**短期记忆网络（LSTM）和门控循环单元（GRU）**等，它们通过引入额外的门控机制来更好地捕捉长距离依赖关系
5. 时序问题的特点与处理方法
   - 数据呈明显的先后特点，时间，字之间先后，需要考虑上下文和前后时序关系
   - 处理方法：捕获神经网络上下文信息
6. 利用最后时刻特征进行输出 
   - 原因: 最后时刻的特征包含了所有前文的信息，是全局的、最全面的。
   - 做法: 使用最后时刻的特征进行预测或分类，作为最终输出。

### RNN存在的问题

- 序列长度过长，难以记住前面的信息，学习效果不佳
- RNN是串行计算，每个时刻都需要等待前一个时刻计算完成，计算速度慢，不适合处理大规模数据
- RNN难以堆叠很多层，每一层都是串联的，层数越多，计算越慢。
- RNN是单向的，只能考虑过去的信息，不能考虑未来的信息。即使双向RNN可以结合左右信息，单并没有解决本质问题
- 对当前时刻影响最大的不一定是最近的时刻，而是全局的某个时刻，RNN无法处理

2017一篇论文提出attention机制，RNN正式退役

![img](img\p-d699f5c9e8d03255dbe926e1f7f87888-40-2025032100-1)

### LSTM网络

**todo**

### 注意机制

**Transformer本质**是一种基于Attention机制的神经网络模型，它通过自注意力机制来捕捉序列中的依赖关系

**Transformer的目的**是为了解决传统RNN网络计算时存在的问题，如输入无法更新，并提供一种高效灵活的序列建模方法。同时为NLP领域提供了新的思路

**人工智能的本质**：解决特征提取的问题，即如何更好的提取和利用特征

传统方法和Transformer对比，传统上需要手动设计算法提取特征，而Transformer则自动地整合和优化特征，使模型学习起来更轻松



#### 自我注意力机制

注意力机制的运作原理

- 决定因素: 注意力机制并不是由人主观决定的，而是由**算法和模型在当前语境下自动分析和判断**的。模型会根据当前的任务和语境，**自动调整对不同部分的关注度**。
- 结合语境: 注意力机制的工作方式是**与当前语境紧密结合**的。**模型会分析语境中的信息**，确定哪些部分是关键的，需要重点关注，哪些部分是不重要的，可以忽视。这种**动态调整的能力使得模型能够更灵活地处理各种复杂的信息**。

![img](img\p-d699f5c9e8d03255dbe926e1f7f87888-40-2025032100-3)

词向量与上下文

![img](img\p-d699f5c9e8d03255dbe926e1f7f87888-40-2025032100-7)

**辅助向量与权重计算**

![企业微信截图_17435609741001](img\企业微信截图_17435609741001.png)

**辅助向量的来源与训练**

![image-20250402103121531](img\image-20250402103121531.png)

#### 注意力机制整体流程

![企业微信截图_17435620803358](img\企业微信截图_17435620803358.png)

**整体流程**

1. 输入词向量x1，x2
2. 通过**全连接层得到辅助向量q1，q2，k1，k2，v1，v2**
3. 计算**q与k的内积**（表示有多匹配），得到权重
4. 使用**softmax进行归一化**，得到最终上下文权重
5. 根据**权重和v向量**，计算得到新的词特征

**softmax和根号dk的作用**

![image-20250402101431538](img\image-20250402101431538.png)

![image-20250402104045858](img\image-20250402104045858.png)

**value向量的作用**

value向量：**表示词自身的特征**，通过加权求和得到新的词特征，例如，新的x1特征为0.5v1 + 0.3v2 + 0.1v3 + 0.1v4

特征更新:通过学习value向量，模型能够更好地理解词自身的特征，从而提升任务性能

**dk的作用**:不让分值随着向量维度的增大而增加

#### 自我注意力机制的计算流程

**多头注意力机制**

1. **多头注意力机制的实现**
   - 实现方法: 在Transformer模型中，多头注意力机制通过**创建多个并行的注意力头**（即多组q, k, v），每个头**使用不同的权重矩阵对输入进行变换**，然后**分别计算注意力得分**，最后**拼接结果作为输出**。
   - 优势: 这种机制允许模型**同时关注输入序列的不同位置**，捕捉到更多的上下文信息。

2. **输入词维度**：输入词维度一般固定，768，由谷歌2018年提出一直沿用

#### **特征不变性**

> - 在多头注意力机制中，每个词的特征表达是不变的，即使调换了词的位置，只要q, k, v不变，权重不变，特征融合的结果也不会变.
>
> - 因为**q, k, v只与词本身有关，与词在句子中的位置无关**
>
> - 在Transformer任务中，还需要**考虑如何**解决特征不变性问题，以提高模型的性能

**位置编码**

1. 位置编码的加法操作: 位置编码通过向量维度上的加法操作与原始的词向量结合,加上对应的位置编码,从而区分了不同位置的信息
2. 位置编码的方法:
   - 非随机初始化: 位置编码一般不是随机初始化的，而是有一定的规律，如正余弦编码或按顺序编码等
3. 强制性:在使用self-attention机制时，必须添加位置编码，因为模型本身对位置信息不敏感，需要额外补充位置信息

**小结**Self-Attention 和Multi-Head Attention

- Self-Attention的计算方法
  - 计算方法: 主要是通过计算**当前词与其他词之间的相关性得分**（或者叫注意力权重），来**对其他词的特征进行加权求和**，从而**得到当前词的表示**。这种机制使得模型能够捕捉到**输入序列中各词之间的依赖关系**，而不仅仅是依赖于词在序列中的位置。
  - Query, Key, Value: 在Self-Attention中，每个词都会生成三个向量：**Query（查询向量）、Key（键向量）和Value（值向量）**。通过计算Query和各个Key的**相似度**（通常用内积），得到**注意力权重，再对Value进行加权求和**，得到最终的输出。
- Multi-Head Attention
  - Multi-Head Attention: Transformer模型不是只做一次**Self-Attention，而是同时做多次**，每次使用不同的Query、Key、Value投影层，得到不同的表示，再将这些表示拼接起来，做一次线性变换得到最终的输出。这样可以允许模型在不同的表示子空间里学习到不同的信息。

#### Transformer中的堆叠

1. **堆叠**Encoder和Decoder层

   - transformer模型不是简单的Self-Attention模块堆叠,，而是由多个Encoder和Decoder层堆叠构成，每个Encoder和Decoder层内部又包含多个Self-Attention和Feed Forward神经网络模块。

2. **Encoder**层:

   - Encoder层: 主要由Self-Attention和Feed Forward两个子层构成，每个子层之后都会接一个Add & Norm模块，即残差连接和层归一化（Layer Normalization），帮助模型训练更加稳定，梯度更容易传播。![image-20250402132142612](img\image-20250402132142612.png)

   - **残差连接**: 解决了深度神经网络中的退化问题，使得深层网络至少不比浅层网络差。
   - 层归一化: 对每个神经元的输入进行归一化，使得输入数据的分布更加稳定，加快模型收敛速度。

3. **Decoder**层详解

   - Decoder层: 与Encoder层类似，但包含第三个子层，用于计算Encoder输出和Decoder输入之间的注意力权重，从而实现序列到序列的生成。
   - Encoder-Decoder Attention: 在Decoder的每个层中，都会计算Decoder的输入与Encoder输出之间的注意力权重，这样Decoder就能利用Encoder的所有信息来生成输出序列。![image-20250402132808133](img\image-20250402132808133.png)

#### 编码解码器

编码器的功能

- **输入与输出**: 编码器的输入是特征，输出也是特征，即**输入一个向量**，输出一个**经过编码的向量**。
- 最终目标: 编码器的最终目标是为了**得到能够预测结果或产出值的特征**，而不仅仅是特征本身。
- 应用场景: 在机器翻译、物理检测等不同的任务中，都需要有输出层来产生最终的结果或值。
- Self Attention的作用: Self Attention在这里被提及作为编码器（encoder）的一部分，负责进行编码层的操作。

解码器功能

  解码器是Transformer模型中的重要组成部分，它通过**Cross Attention和Self Attention机制**，**结合编码器的输出和前文的信息**，**逐步预测**并**输出最终的文本结 果**。同时，Mask机制确保了解码过程的合理性和准确性。

变压器最终输出

- 操作得到预测值，再将预测值经过softmax操作得到概率分布。
- 任务类型: 文本任务通常是分类任务，使用分类函数（如softmax）得到各个类别的概率。
- 视觉任务: 视觉任务可以做全连接、回归或分类，具体任务类型由实际需求决定。

### 整体流程

![image-20250402133328184](img\image-20250402133328184.png)

- 输入与词嵌入
- 位置编码
  - 位置编码: 向量与位置信息不敏感，需加入位置编码以包含位置信息。
- 多头注意力机制
  - 多头注意力机制: 使用多头注意力机制处理向量。
- 归一化与全连接
  - 归一化: 进行归一化处理（如BN）。
  - 全连接: 通过全连接层进一步处理。
- Decoder中的注意力机制
  - Decoder: 在Decoder中，既有自注意力机制（self attention），也有交叉注意力机制（cross attention）。
  - 交替进行: 自注意力和交叉注意力交替进行。
- 多分类输出
  - 多分类: 最终通过全连接层得到特征，进行多分类输出。
- Cross Attention与多模态

### BERT

![image-20250402133737856](img\image-20250402133737856.png)

多模态趋势

- 多模态趋势: 人工智能未来发展必然是多模态的趋势，结合视觉、听觉等多种感官信息，超越单一领域的限制。

BERT模型应用

- 自监督定义: 自监督是一种训练方法，其中模型通过预测自身生成或修改的数据来学习，而不需要外部标注。
- 背景: 由于人工标注成本高昂且有限，自监督方法成为利用大量未标注数据进行学习的有效手段。
- 应用原理: 在NLP中，自监督通过**随机遮挡文本中的部分词汇**，让模型根据上下文预测被遮挡的词汇，从而学习语言模型
- 实例说明: 如GPT等预训练语言模型，通过**自监督方式学习**大量文本数据，提高了模型对语言的理解和生成能力。
- 自监督的具体实现方法：**随机遮挡**（Mask）
- 分词与分字: 在中文自监督训练中，实际应用中更多**采用分字**的方式，因为分词需要额外的分词工具，且分字能更细致地捕捉语言特征,中文自监督训练相比英文更加困难

![image-20250402134014624](img\image-20250402134014624.png)

![image-20250402134038192](img\image-20250402134038192.png)

### 知识小结

| 知识点               | 核心内容                                   | 考试重点/易混淆点                     | 难度系数 |
| -------------------- | ------------------------------------------ | ------------------------------------- | -------- |
| Transformer介绍      | Transformer是特征提取方法                  | Transformer与RNN的区别                | ⭐⭐       |
| RNN问题              | 序列过长、串行计算、层数堆叠限制、单向结构 | RNN无法处理长序列                     | ⭐⭐⭐      |
| Attention机制        | 通过Q、K、V向量计算权重，更新特征          | Q、K、V向量的作用                     | ⭐⭐⭐⭐     |
| Self-Attention       | 词向量与自身及上下文的关系计算             | Self-Attention与位置编码              | ⭐⭐⭐⭐     |
| Multi-Head Attention | 多组Q、K、V向量丰富特征                    | 多头注意力机制的优势                  | ⭐⭐⭐⭐     |
| Position Encoding    | 解决Transformer对位置信息不敏感问题        | 位置编码的方法                        | ⭐⭐       |
| Encoder-Decoder架构  | Encoder提特征，Decoder输出结果             | Encoder与Decoder的交互                | ⭐⭐⭐⭐     |
| Cross-Attention      | Decoder中的词与Encoder特征的关系计算       | Cross-Attention与Self-Attention的区别 | ⭐⭐⭐⭐     |
| Mask机制             | Decoder中词只能看到前文特征                | Mask机制的作用                        | ⭐⭐       |
| Transformer训练      | 自监督学习，如BERT的预训练任务             | BERT的预训练任务类型                  | ⭐⭐⭐      |
| 应用场景             | NLP、CV、多模态等                          | Transformer在不同领域的应用           | ⭐⭐       |

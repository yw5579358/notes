# 深度学习CORE

## 神经网络

### 前向传播

> 前向传播是神经网络从输入层到输出层逐层计算预测值的过程。数据通过权重矩阵、偏置向量和激活函数的组合，最终得到预测结果。

**步骤**：

1. **输入层**：接收原始输入数据 `X`。
2. **线性变换**：每一层计算加权和 `Z = W·A_prev + b`  
   - `W`：权重矩阵  
   - `A_prev`：前一层输出（输入层为 `X`）  
   - `b`：偏置向量  
3. **激活函数**：应用非线性激活函数（如ReLU、Sigmoid）  `A = σ(Z)`  
4. **逐层传递**：重复上述步骤，直到输出层得到预测值 `y_pred`。

**作用**

- 生成预测结果。
- 为反向传播提供中间计算结果（如 `Z`, `A`）。

---

### 反向传播

> 反向传播通过**链式法则**计算损失函数对模型参数的梯度，从输出层向输入层逐层更新权重和偏置。

**步骤**

1. **计算损失**：根据预测值 `y_pred` 和真实值 `y_true` 计算损失 `L`（如**MSE**、**交叉熵**）。
2. **输出层梯度**：计算损失对输出层输入的梯度 `dZ_output = dL/dy_pred * σ'(Z_output)`。
3. **链式求导**：逐层反向计算梯度：  
   - 权重梯度：`dW = (dZ)·A_prev.T / m`  
   - 偏置梯度：`db = sum(dZ, axis=1) / m`  
   - 前一层梯度：`dA_prev = W.T·dZ`  
   - 激活梯度：`dZ_prev = dA_prev * σ'(Z_prev)`  （`m` 为样本数）
4. **更新参数**：使用优化器（如**SGD、Adam**）根据梯度更新权重和偏置：  
   `W = W - η·dW`  
   `b = b - η·db`  
   （`η` 为学习率）

**作用**

- 逐步计算参数梯度，指导模型优化方向(梯度下降最快)。
- 通过最小化损失函数，提升模型预测精度。

核心思想:梯度下降，通过迭代逐步优化权重，每次更新一小步。

### 前向反向传播对比

| **特性**       | **前向传播**                 | **反向传播**                 |
| -------------- | ---------------------------- | ---------------------------- |
| **方向**       | 输入层 → 输出层              | 输出层 → 输入层              |
| **主要计算**   | 线性变换 + 激活函数          | 链式求导 + 梯度计算          |
| **目的**       | 生成预测结果                 | 优化模型参数                 |
| **存储中间值** | 保存 `Z` 和 `A` 用于反向传播 | 复用前向传播的中间值计算梯度 |

![image-20250401110719792](img\image-20250401110719792.png)

- 层次结构: 神经网络由输入层、隐藏层、输出层组成，形成层次化的特征提取过程。
- 权重参数: 在神经网络中，用于将输入层的特征转换成隐层特征值的系数称为权重参数。
- 神经元: 每个圆圈代表一个神经元，可视为一个特征提取器。
- 全连接: 每一层的神经元与下一层的所有神经元相连，实现信息的全面传递，通过多个隐层逐步**提取出更深层次的特征**
- 非线性: 引入激活函数等非线性元素，使网络能够拟合更复杂的函数关系。

学习与更新: 权重参数不是随机的，而是通过模型的学习过程来确定的。模型会根据预测结果与实际标签的差距（损失）来调整权重参数，使得预测结果越来越接近实际标签。目标: **权重参数的学习与更新的目标是使得神经网络能够更好地拟合数据**，即预测结果与实际标签的差距最小化。

非线性: 线性操作与替换**问题**:神经网络的每一层都可以看作是一个线性操作,(wx+b)如果将多层神经网络的权重参数矩阵连续相乘，得到一个总的权重参数矩阵，那么神经网络就**退化成了一个简单的线性模型**,因此引入**激活函数**，使得每一层的**输出都是非线性**的，从而能够学习到**更加复杂的特征表示**

激活函数:

- Sigmoid函数,Sigmoid函数在早期神经网络中广泛使用，但存在梯度消失问题，当输入值较大或较小时，**梯度**接近于0(梯度消失)，导致网络难以更新
- ReLU函数:ReLU函数因其通用性和高效性，在大多数情况下是首选。

Dropout技术：每一层随机丢弃一些神经元

## 卷经济神经网络CNN

>  图像数据通常具有⾼维度,300x300,3个颜⾊通道，则输⼊层有27万个神经元,仅第⼀层就有27万 ×128个参数，**参数量巨⼤**

**卷积神经⽹络处理图像数据的优势:**

- 局部连接,权值共享: 卷积神经⽹络通过**卷积层、池化层**等结构,进行局部连接和权值共享，⼤⼤**减少了参数量**，使得⽹络更加轻便。
- 适合图像特征提取: 卷积层能够**提取图像中的局部特征**，并通过**多层卷积**逐渐提取更**⾼级的特征**，⾮常适合处理图像数据。
- 平移不变性：卷积神经⽹络对图像的平移具有⼀定的鲁棒性（**异常输入、干扰、不确定性或意外条件**‌时，仍能保持稳定运行或维持预期性能的能力）
- 空间关系保留: 卷积神经⽹络能够**保留图像中的空间关系**，使得⽹络能够更好地理解图像内容

### 网络架构

- 输⼊层: 接收**原始图像数据**。
- 卷积层: 对输⼊图像进⾏**卷积操作，提取特征**。
- 池化层: 对卷积层输出的特征图进⾏**下采样**，**降低特征图的维度**。
- 全连接层: 将池化层**输出的特征图展平成⼀维向量**，并通过**全连接⽹络进⾏分类或回归**。
- 输出层: **输出最终的分类结果或回归值**，通常使⽤**Softmax激活函数进⾏多分类**。

![image-20250401145653924](img\image-20250401145653924.png)

卷积核定义:卷积核(filter)在图像处理中用于提取特定区域的特征，常见卷积核3*3核5*5

特征图（feature map）是通过卷积核在输⼊图像上滑动并计算得到的特征点组合

卷积核与通道: 卷积核的通道数必须与输⼊图像的通道数相匹配，例如RGB图像有三个通道，卷积核也应有三个通道

卷积目的: 为了降低计算量，我们将图像**切分成多个区域**，**每个区域计算⼀个特征**，从⽽**减少计算量并更⼴泛地考虑图像信息**

**卷积计算过程:**

- 内积计算：卷积核与输⼊图像对应区域的计算实质上是**内积运算**，即对应位置**相乘后求和**
- 偏置项: 在内积计算的结果上加上**偏置项**，得到最终的特征值
- 参数共享：卷积核的**权重在滑动过程中是共享**的，即在同⼀输⼊图像上，卷积核的权重保持不变。
- 滑动步长: 卷积核在输入图像上滑动的**步长影响特征图的尺寸核特征提取的细致程度**，步长越小，特征取值越细致

![image-20250401145934161](img\image-20250401145934161.png)

步⻓推荐值: 在常规任务中，**步⻓通常设为1**，因为1是最⼩的步⻓，能够确保遍历整个输⼊区域
卷积核个数的选择:理论上，卷积核个数越多越好,因为能获取更多信息,常见:256,512。

卷积操作的公平性:

- 不公平性的表现：卷积操作对输⼊数据的**不同位置并不是公平的**，**边缘区域的值被卷积的次数较少，⽽中⼼区域的值被卷积的次数较多**
- 不公平性的影响: 边缘区域的值对最终特征的影响较⼩，⽽**中⼼区域的值对最终特征的影响较⼤**
- 解决⽅法: 引⼊**padding（填充）来平衡边缘和中⼼区域被卷积的次数**

![image-20250401152629571](img\image-20250401152629571.png)

### **卷积层数与感受野**

多层卷积有助于提取图像中不同层次的特征

**感受野**是指在卷积神经⽹络中，**某⼀层输出特征图上的某个像素点所对应的输⼊图像上的区域⼤⼩**

随着卷积层数的增加，感受野逐渐增⼤，意味着⽹络能够捕捉到更全局的信息.**深层网络能捕捉更高级，更全局的特征**

卷积层的选择根据具体任务而定，深层网络适合大目标检测，但可能无法有效检测小目标

![image-20250401171946875](img\image-20250401171946875.png)

![image-20250401172153411](D:\wym\文档\notes\image-20250401172153411.png)

### 池化层的定义与作用

- 池化层通常也被称为下采样层

- 减少特征的数量，降低计算复杂度,提⾼计算速度,
- 提取最具代表性的特征，增强模型的泛化能⼒
- 去除不必要的特征，减少过拟合风险
- 有平均池化和最大池化

### 网络结构中的层

**⽹络结构的层**: 在任务中，⽹络结构通常包括输⼊层、卷积层、激活函数层、池化层等。这些层组合在⼀起，形成了⼀种**循环或重复的结构，即block**。每个block内部可能包含多个卷积、⾮线性操作和池化等操作

### 全连接层

> 特征图虽然包含了数据的特征信息，但并不能直接⽤于分类.将特征图拉⻓成⼀个⼀维向量,接着，通过全连接层,对拉⻓后的向量进⾏处理，输出各个类别的概率,计算可以表示为WX + b.W是权重矩阵，X是拉⻓后的特征向量，b是偏置项

作用:全连接层（FC）主要负责整合卷积层提取的特征，并输出最终的结果或预测值。

位置:全连接层通常位于神经⽹络的末端，紧跟在卷积层、池化层等特征提取层之后

### 批量标准化BN

**BN技术**: BN（Batch Normalization）技术，它通过在每个⼩批量数据上进⾏归⼀化操作，使得数据的分布更加稳定，从⽽加速了神经⽹络的训练过程，提⾼了模型的收敛速度和性能

**批量标准化原理**：BN通常与卷积绑定使用，BN相当于在卷积之后再次进⾏标准化操作，它会对特征进⾏微调，使离群的特征点向中⼼靠拢，但不影响⼤部分正常特征

**可学习性**:BN层并不是简单地应⽤⼀个固定的标准化公式，⽽是根据数据的特点和分布进⾏适当程度的拉回操作,动态调整标准化的程度,这种学习是自动的

## Transformer

> RNN串行计算,难以堆叠,单向在处理长序列时可能会遇到**梯度消失或梯度爆炸**的问题，导致无法有效捕捉长距离依赖关系
>
> **长短期记忆网络（LSTM）和门控循环单元（GRU）**等，它们通过引入额外的门控机制来更好地捕捉长距离依赖关系

### 自注意力机制

**Transformer本质**是一种基于Attention机制的神经网络模型，为了解决传统RNN网络计算时存在的问题

意力机制并不是由人主观决定的，而是由**算法和模型在当前语境下自动分析和判断**的。注意力机制的工作方式是**与当前语境紧密结合**的。**模型会分析语境中的信息**，确定哪些部分是关键的，需要重点关注，哪些部分是不重要的，可以忽视。这种**动态调整的能力使得模型能够更灵活地处理各种复杂的信息**。

### Self-attention的计算

![企业微信截图_17435620803358](img\企业微信截图_17435620803358.png)

![image-20250402101431538](img\image-20250402101431538.png)

**整体流程**

1. 输入词向量x1，x2 (z1为结果，表示在考虑了不同片段关系后的结果)
2. 通过**全连接层得到辅助向量q1，q2，k1，k2，v1，v2**（v表示词自身的特征，通过加权求和得到新词的特征，如0.5v1 + 0.3v2 + 0.1v3 + 0.1v4，通过v学习可以更好理解词自身的特征）
3. 计算**q与k的内积**（表示有多匹配），得到权重
4. 使用**softmax进行归一化**，得到最终上下文权重
5. 根据**权重和v向量**，计算得到新的词特征

**dk的作用**:不让分值随着向量维度的增大而增加

![image-20250402104045858](img\image-20250402104045858.png)

### **多头注意力机制**

实现方法: 在Transformer模型中，多头注意力机制通过**创建多个并行的注意力头**（即多组q, k, v），每个头**使用不同的权重矩阵对输入进行变换**，然后**分别计算注意力得分**，最后**拼接结果作为输出**。这种机制允许模型**同时关注输入序列的不同位置**，捕捉到更多的上下文信息。

**输入词维度**：输入词维度一般固定，768，由谷歌2018年提出一直沿用

### **位置编码**

> 位置编码通过向量维度上的加法操作与原始的词向量结合,加上对应的位置编码,从而区分了不同位置的信息

1. 位置编码的方法:非随机初始化: 位置编码一般不是随机初始化的，如正余弦编码或按顺序编码等
2. 强制性:在使用self-attention机制时，必须添加位置编码，因为模型本身对位置信息不敏感，需要额外补充位置信息

### ⭐总结 Self-Attention 和Multi-Head Attention

- Self-Attention的计算方法
  - 计算方法: 主要是通过计算**当前词与其他词之间的相关性得分**（或者叫注意力权重），来**对其他词的特征进行加权求和**，从而**得到当前词的表示**。这种机制使得模型能够捕捉到**输入序列中各词之间的依赖关系**，而不仅仅是依赖于词在序列中的位置。
  - Query, Key, Value: 在Self-Attention中，每个词都会生成三个向量：**Query（查询向量）、Key（键向量）和Value（值向量）**。通过计算Query和各个Key的**相似度**（通常用内积），得到**注意力权重，再对Value进行加权求和**，得到最终的输出。
- Multi-Head Attention
  - Multi-Head Attention: Transformer模型不是只做一次**Self-Attention，而是同时做多次**，每次使用不同的Query、Key、Value投影层，得到不同的表示，再将这些表示拼接起来，做一次线性变换得到最终的输出。这样可以允许模型在不同的表示子空间里学习到不同的信息。

### Transformer中的堆叠

**堆叠**Encoder和Decoder层

- transformer模型不是简单的Self-Attention模块堆叠,，而是由多个Encoder和Decoder层堆叠构成，每个Encoder和Decoder层内部又包含多个Self-Attention和Feed Forward神经网络模块。

**Encoder**层:

- Encoder层: 主要由Self-Attention和Feed Forward两个子层构成，每个子层之后都会接一个Add & Norm模块，即残差连接和层归一化（Layer Normalization），帮助模型训练更加稳定，梯度更容易传播。

![image-20250402132142612](img\image-20250402132142612.png)

- **残差连接**: 解决了深度神经网络中的退化问题，使得深层网络至少不比浅层网络差。
- 层归一化: 对每个神经元的输入进行归一化，使得输入数据的分布更加稳定，加快模型收敛速度。

**Decoder**层详解

- Decoder层: 与Encoder层类似，但包含第三个子层，用于计算**Encoder输出和Decoder输入之间的注意力权重**，从而实现**序列到序列的生成**。
- **Encoder-Decoder Attention**: 在Decoder的每个层中，都会计算**Decoder的输入与Encoder输出之间的注意力权重**，这样**Decoder就能利用Encoder的所有信息来生成输出序列**。

### ![image-20250402132808133](img\image-20250402132808133.png)编码解码器

编码器的功能

- **输入与输出**: 编码器的输入是特征，输出也是特征，即**输入一个向量**，输出一个**经过编码的向量**。
- 最终目标: 编码器的最终目标是为了**得到能够预测结果或产出值的特征**，而不仅仅是特征本身。
- 应用场景: 在机器翻译、物理检测等不同的任务中，都需要有输出层来产生最终的结果或值。
- Self Attention的作用: Self Attention在这里被提及作为编码器（encoder）的一部分，负责进行编码层的操作。

解码器功能

  解码器是Transformer模型中的重要组成部分，它通过**Cross Attention和Self Attention机制**，**结合编码器的输出和前文的信息**，**逐步预测**并**输出最终的文本结 果**。同时，Mask机制确保了解码过程的合理性和准确性。

变压器最终输出

- 操作得到预测值，再将**预测值经过softmax操作得到概率分布**。
- 任务类型: 文本任务通常是分类任务，使用分类函数（如softmax）得到各个类别的概率。
- 视觉任务: 视觉任务可以做全连接、回归或分类，具体任务类型由实际需求决定。

### ⭐整体流程

![image-20250402133328184](img\image-20250402133328184.png)

- 输入与词嵌入
- 位置编码
  - 位置编码: 向量与位置信息不敏感，需加入位置编码以包含位置信息。
- 多头注意力机制
  - 多头注意力机制: 使用多头注意力机制处理向量。
- 归一化与残差连接
  - 归一化: 进行归一化处理（如BN 批归一化，LN层归一化）
  - 残差连接，防止深层网络导致遗忘。
- 全连接: 通过全连接层进一步处理。
- Decoder中的注意力机制
  - Decoder: 在Decoder中，既有自注意力机制（self attention），也有交叉注意力机制（cross attention）。
  - 交替进行: 自注意力和交叉注意力交替进行。
- 多分类输出
  - 多分类: 最终通过全连接层得到特征，进行多分类输出。
- Cross Attention与多模态
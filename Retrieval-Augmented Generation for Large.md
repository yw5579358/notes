# 大型检索增强生成语言模型（RAG）综述学习笔记

> 📅 日期：2025年6月27日
> 📚 论文：A Survey of Retrieval-Augmented Generation (arXiv:2312.10997v5)
> 作者：Haofen Wang 等（同济大学、复旦大学）

## 一、背景与动机

### 大型语言模型的局限性

尽管LLM（如GPT-4）在众多NLP任务中取得显著成果，但依然面临关键挑战：

- **幻觉（Hallucination）**：生成事实错误内容；
- **知识滞后**：无法访问最新信息；
- **推理不透明**：无法解释生成的依据；
- **专业领域适配性差**：对医学、法律等领域常常理解有限。

### RAG的核心价值

RAG（Retrieval-Augmented Generation）引入外部知识库，结合LLM内在知识，形成如下机制：

1. 从外部数据库/文档中检索语义相关内容；
2. 将检索内容与原始问题共同输入LLM；
3. LLM依据更丰富上下文生成答案。

> ✅ **目标**：提升回答准确性，增强对新/特定领域知识的覆盖。

## 二、RAG范式演化路径

RAG的研究从朴素形式逐步发展至高级与模块化形态。

### 1. 朴素 RAG（Naive RAG）

#### 结构：

- **索引**：文档切分、向量化后存入向量数据库；
- **检索**：根据查询语义向量相似度选择Top-K块；
- **生成**：将查询+检索块组成提示，交由LLM生成回答。

#### 优点：

- 简单直接，易于部署；
- 相较纯LLM提升准确性。

#### 局限：

- 无法处理复杂问题；
- 检索粒度固定，缺乏策略性；
- 容易引入冗余或无关信息。

### 2. 高级 RAG（Advanced RAG）

#### 核心改进方向：

- **索引优化**：使用滑动窗口、细粒度切块、元数据注解；
- **查询优化**：包括重写、转换、扩展（如HyDE）等策略；
- **检索后处理**：重排序、上下文压缩、结构化表示等。

#### 实例：

- LlamaIndex 实现语义重排序与压缩；
- 多源查询（multi-query）提高召回率。

### 3. 模块化 RAG（Modular RAG）

#### 关键特征：

- 支持自定义模块替换与插拔；
- 构建跨组件处理流程，如：
  - 检索器 + re-ranker + 上下文规划器 + LLM；
  - 多模态检索（KG/表格/SQL）；

#### 创新模块：

- 搜索模块：支持多源异构知识接入（SQL、API、KG）；
- 记忆模块：LLM可存取历史检索内容形成记忆池（如SelfMem）；
- 控制模块：通过检索判断模块自动决定是否发起检索（如Self-RAG）。

> 模块化RAG的最大优势是灵活适配不同任务、数据类型与模型架构。

## 三、检索（Retrieval）技术细节

### A. 索引优化

#### 分块策略：

- 固定长度切分（token数）；
- 递归拆分+滑动窗口（保持语义完整性）；
- Small2Big：从句子级单位出发，合并上下文。

#### 元数据增强：

- 页码、文档ID、类别、时间戳；
- 利用元信息进行查询过滤、路由。

#### 分层索引结构：

- 构建父子文档层级；
- 每层节点含摘要向量；
- 用于快速遍历与块定位。

#### 知识图谱索引：

- 使用KG建立段落间的显式连接（边/节点）；
- 支持子图检索，提高上下文一致性。

### B. 查询优化

#### 查询重写：

- 使用LLM改写用户原始查询以提升可检索性；
- 专用小模型如RRR（Rewriting-Retrieval-Reading）用于商用系统。

#### 查询扩展：

- 基于Prompt或语义生成多个视角查询；
- 多查询并发检索，提高Recall。

#### 查询转换（如HyDE）：

- LLM生成假设答案；
- 用答案向量搜索相关段落。

#### 查询路由：

- 根据语义或元数据将查询发送至不同检索路径（如多通道检索管线）。

### C. 嵌入模型选择

#### 密集 vs 稀疏：

- 稀疏模型如 BM25；
- 密集模型如 BERT、BGE、AngIE 等（支持跨语言、跨任务）。

#### 混合嵌入：

- 稀疏/密集嵌入互补使用，提高覆盖率。

#### 微调策略：

- 微调嵌入模型以适配特定领域（医疗、法律等）；
- 支持软监督（LLM反馈）与硬标签训练（如REPLUG）。

## 四、增强机制详解（Augmentation）

### A. 语境策划

#### 问题：

- 太多上下文会“稀释”模型注意力；
- LLM对中间信息处理能力弱。

#### 策略：

- 重排序（规则或模型驱动）；
- 压缩（删除无关token，保留核心信息）；
- 规划（基于任务选择合适上下文组合）。

### B. 检索增强策略分类

#### 1. 迭代检索（Iterative RAG）

- 检索-生成循环执行；
- 每轮基于当前生成结果优化检索。

#### 2. 递归检索（Recursive RAG）

- 将问题拆分为子问题；
- 分别检索/生成，汇总整合最终回答。

#### 3. 自适应检索（Adaptive RAG）

- LLM判断是否需要检索，何时触发；
- 使用标记控制（如“<检索>”），或置信度机制。

#### 示例工具：

- Self-RAG：无需额外分类器，靠标记自主判断是否检索；
- WebGPT：引导模型自主使用搜索引擎。

## 五、生成阶段优化（Generation）

### A. Prompt设计

- 将检索块+用户查询组织成合适的提示结构；
- 可包含多轮对话历史，强化上下文理解。

### B. LLM微调

- 对生成模型微调使其更适应增强型输入；
- 使用对比损失、KL对齐等技术协调检索器和生成器。

### C. 答案控制与过滤

- 让LLM参与对检索信息的打分与过滤；
- 筛选或权重化上下文，提高生成相关性。

## 六、RAG未来趋势

### 🌱 多模态RAG

- 接入图像、表格、结构化数据
- 跨模态检索与理解（如 Text2SQL、KG-RAG）

### 🔧 工具链生态

- LangChain、LlamaIndex、Haystack
- Flowise AI（低代码）、Amazon Kendra（企业级）

### 🧠 与SLM结合

- 使用轻量级语言模型进行预检索/过滤；
- 降低计算负担，提升鲁棒性（如Filter-Reranker框架）。

### 🚀 自动推理与Agent整合

- Graph-Toolformer、AutoGPT等模型支持主动检索、自动规划子任务；
- LLM不再只是被动响应器，而是主动推理Agent。

> 🔗 相关资源：
>
> - [RAG综述项目主页](https://github.com/Tongji-KGLLM/RAG-Survey)
> - [LlamaIndex](https://www.llamaindex.ai) | [LangChain](https://www.langchain.com)

|      |      |
| ---- | ---- |
|      |      |
|      |      |
|      |      |
|      |      |
|      |      |
|      |      |